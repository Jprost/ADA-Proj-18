{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nicolasgandar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nicolasgandar/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet, brown, words\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from bokeh.plotting import *\n",
    "from bokeh.models import *\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.palettes import Category20\n",
    "\n",
    "nltk.download('stopwords')  \n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='Data/russian-troll-tweets/IRAhandle_tweets_'\n",
    "pickle_files='Pickles/'\n",
    "WORD_FREQ = 'Data/wordfrea.xlsx'\n",
    "plot_files='Plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This part focuses on the textual content and representation of the tweets. The aim is to characterize the typical tweet of each troll category. First, the tweets are cleaned, a vocabulary as well as a colelction of hash-tags are build. Then a Word2Vec model is used to gather words by 'concepts'. Finally, each tweet is represented by the words it contains. A classifier is build to distinguish right and left trolls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already pretty clean. We Select only the 'LeftTroll' and 'RightTroll' tweet after 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(x):\n",
    "    \"\"\"From string to datatime.date()\"\"\"\n",
    "    return dt.datetime.strptime(x, '%m/%d/%Y %H:%M').replace(minute=0, hour=0, second=0)\n",
    "\n",
    "def cleaning_content(df,  right=True, left=True):\n",
    "    df_tmp=df[df.language=='English'].drop(columns={'harvested_date', 'language'})\n",
    "    df_tmp=df.loc[:,['publish_date','content', 'account_category', 'author']]\n",
    "\n",
    "    #category\n",
    "    if not left:\n",
    "        df_tmp=df_tmp[(df_tmp.account_category=='RightTroll')]\n",
    "    elif not right:\n",
    "        df_tmp=df_tmp[(df_tmp.account_category=='LeftTroll')]\n",
    "    else:        \n",
    "        df_tmp=df_tmp[(df_tmp.account_category=='RightTroll') | (df_tmp.account_category=='LeftTroll')  ]\n",
    "\n",
    "    #date \n",
    "    df_tmp['publish_date']=df_tmp.publish_date.apply(lambda x: get_date(x))\n",
    "    #selecting after Oct 2014\n",
    "    df_tmp=df_tmp[df_tmp.publish_date > dt.datetime(2014,10,1,0,0,0)]\n",
    "\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data and merging (9) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "for dataset in range(1,9): #loading every file \n",
    "    df_tmp=cleaning_content(pd.read_csv(data+str(dataset)+'.csv'))\n",
    "    df=df.append(df_tmp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of different trolls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 850 trolls: 620 right and 230 left.\n"
     ]
    }
   ],
   "source": [
    "print('There are {} trolls: {} right and {} left.'.format(df.author.unique().shape[0],\\\n",
    "                                                          df[df.account_category=='RightTroll'].author.unique().shape[0],\\\n",
    "                                                          df[df.account_category=='LeftTroll'].author.unique().shape[0]))\n",
    "df.drop(columns='author', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1131960 tweets, 935229 are unique (196731 tweets are duplicated).\n"
     ]
    }
   ],
   "source": [
    "print('From {} tweets, {} are unique ({} tweets are duplicated).'\\\n",
    "      .format(df.shape[0],df.content.unique().shape[0],df.shape[0]-df.content.unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1131960 tweets. 713177 Right-trolls and 418783 Left-trolls.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>content</th>\n",
       "      <th>account_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publish_date                                            content  \\\n",
       "0   2017-10-01  \"We have a sitting Democrat US Senator on tria...   \n",
       "1   2017-10-01  Marshawn Lynch arrives to game in anti-Trump s...   \n",
       "2   2017-10-01  Daughter of fallen Navy Sailor delivers powerf...   \n",
       "3   2017-10-01  JUST IN: President Trump dedicates Presidents ...   \n",
       "4   2017-10-01  19,000 RESPECTING our National Anthem! #StandF...   \n",
       "\n",
       "  account_category  \n",
       "0       RightTroll  \n",
       "1       RightTroll  \n",
       "2       RightTroll  \n",
       "3       RightTroll  \n",
       "4       RightTroll  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_left=df[df.account_category=='LeftTroll'].shape[0]\n",
    "nb_right=df[df.account_category=='RightTroll'].shape[0]\n",
    "\n",
    "print('There are {} tweets. {} Right-trolls and {} Left-trolls.'\\\n",
    "      .format(df.shape[0], nb_right, nb_left))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The text is process as follow:\n",
    "* hastags are exctracted (analysis to be done ...)\n",
    "* the text is cleaned from punctuation (execpt '-')\n",
    "* words are set to lower keys\n",
    "* words that are not written with 'latin' caracters (emojiis, foreign alphabet...) are removed\n",
    "* words that start/end with number are discarder\n",
    "* links are discarded (start with 'http...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_numbers=('http', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-') # symbole that we want to avoid\n",
    "stemmer = PorterStemmer() #stemmer\n",
    "#stop words and 'rt'= retweet, 'amp'= encoding for @ \n",
    "stop_w=[word.replace('\\'','') for word in stopwords.words('english')]+ ['', '&amp', 'amp','rt']\n",
    "\n",
    "#load data frequency from http://www.wordfrequency.info\n",
    "wordfrequency = pd.read_excel(WORD_FREQ, header=0, index_col=[0], usecols=3).dropna()\n",
    "wordfrequency.Frequency = wordfrequency.Frequency/(wordfrequency.Frequency.sum())\n",
    "wordfrequency.rename(columns={'\\xa0\\xa0\\xa0Word' : 'Word', 'Part of speech' : 'PoS'}, inplace=True)\n",
    "wordfrequency.Word = wordfrequency.apply(lambda row: row['Word'].replace(\"\\xa0\\xa0\\xa0\", ''), axis=1)\n",
    "\n",
    "word_dictionary = list(set(words.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Split the the tweet into a list of (cleaned words). The hashtags are treated separatly\n",
    "    and splitted in words\"\"\"\n",
    "    \n",
    "    text_cleaned = ''.join(ch for ch in text if ch not in '!\"$%&\\()*+,./:;<=>?@[\\\\]^_{|}~\\'').split(' ') \n",
    "    \n",
    "    #take the lower key, the english symbols and removing numbers\n",
    "    #words_cleaned= [word.lower().encode('ascii',errors='ignore').decode() for word in text_cleaned \\\n",
    "    #        if not ( (word.startswith(link_numbers)) | (word.endswith(link_numbers))) ]\n",
    "    \n",
    "    #stemming words that are not in stop words\n",
    "    words=[]\n",
    "    for word in text_cleaned:   \n",
    "        if word.startswith('#'): #treat hastags separatly\n",
    "            if len(word)>1:# if is only a '#' charater, do not save it\n",
    "                \n",
    "                #Splitt the hastags \n",
    "                raw_hashtag=word.replace('#', '').lower().encode('ascii',errors='ignore').decode()\n",
    "                words= words + hashtag_splitter(word) +  [stemmer.stem(raw_hashtag)]\n",
    "        \n",
    "        else: #words\n",
    "            word_tmp=word.lower().encode('ascii',errors='ignore').decode() # take only english symbols\n",
    "            \n",
    "            #get rid of numbers characters and stop words\n",
    "            if (not (word_tmp.startswith(link_numbers)) | (word_tmp.endswith(link_numbers))) \\\n",
    "                    & (word_tmp not in stop_w):\n",
    "                words.append(stemmer.stem(word_tmp))#.lower().encode('ascii',errors='ignore').decode()))\n",
    "\n",
    "    #words=[stemmer.stem(word.lower().encode('ascii',errors='ignore').decode()) for word in words \\\n",
    "           # if not ( (word.startswith(link_numbers)) | (word.endswith(link_numbers))) & (word not in stop_w) ]\n",
    "    \n",
    "    if len(words) > 0: #if the list of words is not empty\n",
    "        return words\n",
    "    else: #tweets that contains only links or emojiis ... that are empty\n",
    "        pass\n",
    "        \n",
    "def hashtag_splitter(word):\n",
    "    '''\n",
    "    Receives an hashtag, removes '#' and check if the word is in dic.\n",
    "    If not, try to split it according to uppercases.\n",
    "    E.g: #BlackLivesMatters --> black, live, matter\n",
    "         #NFLProtest --> nfl, protest\n",
    "         #Stemming --> stem\n",
    "    '''\n",
    "    new_wordlist=[]\n",
    "    hashtag = word.replace('#', '')\n",
    "    \n",
    "    #Is the hashtag a word itself?\n",
    "    if (hashtag.lower() in word_dictionary):\n",
    "        new_wordlist.append(stemmer.stem(hashtag.lower()))\n",
    "    \n",
    "    #If not, let's split it    \n",
    "    else:\n",
    "        split = []\n",
    "        upper = []\n",
    "        new_word = []\n",
    "        \n",
    "        #going over each char in the hashtag\n",
    "        for idx, char in enumerate(hashtag):\n",
    "           \n",
    "            #If char is uppercase, store it in potential word beggining\n",
    "            if char.isupper():\n",
    "                upper.append(char)\n",
    "                #Upper means potentially the end of a word, if this is the case, store ex-new_word\n",
    "                if len(new_word) !=0:\n",
    "                    split.append(''.join(new_word))\n",
    "                    new_word = []\n",
    "            else:\n",
    "                #if char is not uppercase, not precedated by uppercase\n",
    "                if len(upper) == 0:\n",
    "                    new_word.append(char)\n",
    "                    #end of hashtag\n",
    "                    if idx == (len(hashtag)-1):\n",
    "                        split.append(''.join(new_word))\n",
    "                else:\n",
    "                    #If there was an ongoing uppercased word, we save it and start a new word with the \n",
    "                    # previous uppercased char as first char of new word\n",
    "                    if len(upper) != 0:\n",
    "                        split.append(''.join(upper[:-1]))\n",
    "                        new_word.append(upper[-1])\n",
    "                        new_word.append(char)\n",
    "                        upper = []\n",
    "                    else:\n",
    "                        print('Error')\n",
    "\n",
    "        new_wordlist = new_wordlist + [stemmer.stem(word.lower().encode('ascii',errors='ignore').decode()) for word in split if word.lower() not in \\\n",
    "                (wordfrequency[:150].Word.values.tolist() + [''] + ['i'] + stop_w)]\n",
    "    return new_wordlist\n",
    "\n",
    "def build_vocab(tweets, min_occurence=10):\n",
    "    \"\"\"Gathers all the words that are more frequent than min_occurence\"\"\"\n",
    "    \n",
    "    filename=pickle_files+'vocabulary_'+str(min_occurence)+'.pkl'\n",
    "\n",
    "    voc_raw=[]\n",
    "    for words in tweets:\n",
    "        if words: #if not None\n",
    "            voc_raw+=words # add to voca\n",
    "    count_=Counter(voc_raw) #count frequency\n",
    "    \n",
    "    vocabulary=[word for word in count_.keys() if count_[word] >= min_occurence]\n",
    "    \n",
    "    with open(filename, 'wb') as voc:\n",
    "            pickle.dump(vocabulary, voc, pickle.HIGHEST_PROTOCOL)\n",
    "    print('Vocabulary saved -->')\n",
    "    \n",
    "    return vocabulary\n",
    "\n",
    "\n",
    "def get_vocabulary(tweets, min_count=10):\n",
    "    \"\"\"Recovers the vocabulary from pickles\"\"\"\n",
    "    filename=pickle_files+'vocabulary_'+str(min_count)+'.pkl'\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'rb') as voc:\n",
    "            vocabulary=pickle.load(voc) \n",
    "        print(\"Vocab loaded <---\")\n",
    "            \n",
    "    except: \n",
    "        vocabulary= build_vocab(tweets)\n",
    "        \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8158a9184842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#list of words for each tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ada/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8158a9184842>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(content)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Clean_tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#list of words for each tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-7e9de7d19353>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;31m#Splitt the hastags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mraw_hashtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhashtag_splitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_hashtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7e9de7d19353>\u001b[0m in \u001b[0;36mhashtag_splitter\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m#Is the hashtag a word itself?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhashtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mnew_wordlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['Clean_tweet']=df.content.apply(lambda content: tokenize(content))#list of words for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the vocabulary \n",
    "vocabulary=get_vocabulary(df.Clean_tweet, min_count=10)\n",
    "vocabulary=build_vocab(df.Clean_tweet, min_occurence=10)\n",
    "print('Vocabulary is {} long'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out elements less frequent than 10 times\n",
    "df['Clean_tweet']=df.Clean_tweet.apply(lambda list_word: [word for word in list_word if word in vocabulary]\\\n",
    "                                                           if list_word else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtags #ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hastag_collection(text, nb_min=10):\n",
    "    \"\"\"Defines the collection of hastags in the entire dataset. \n",
    "    Try to load the hastag list if its available\"\"\"\n",
    "    \n",
    "    filename=pickle_files+'hastag_collection_count_'+str(nb_min)+'.pkl'\n",
    "    \n",
    "    try :\n",
    "        with open(filename, 'rb') as h:\n",
    "            count_=pickle.load(h) \n",
    "        print('Pickle loaded <--')\n",
    "            \n",
    "    except:\n",
    "        hashtags = []\n",
    "        text = text.split()\n",
    "        for words in text:\n",
    "            new_hash = re.match(\"#[A-Za-z0-9\\-\\.\\_]+\", words)\n",
    "            if new_hash:\n",
    "                hashtags.append(new_hash.group(0))\n",
    "        count_=Counter(hashtags)\n",
    "\n",
    "        with open(filename, 'wb') as h:\n",
    "            pickle.dump(count_, h, pickle.HIGHEST_PROTOCOL) #saving the tweet_vect\n",
    "        \n",
    "    return [hashtag for hashtag in count_.keys() if count_[hashtag] >= nb_min]\n",
    "\n",
    "def hashtag_extractor(text,hastag_coll):\n",
    "    \"\"\"Extract hastags if they happend more than nb_min times (over the whole data)\"\"\"\n",
    "    hashtags = []\n",
    "    for word in text.split():\n",
    "        if (word.startswith('#')) & (word.encode('ascii',errors='ignore').decode() in hastag_coll):\n",
    "            hashtags.append(word.encode('ascii',errors='ignore').decode())\n",
    "    return hashtags\n",
    "\n",
    "def hastag_orientation(df, hashtags_collec):\n",
    "    \"\"\"Defines if a words have been used more by Right or Left trolls.\n",
    "    LeftTroll: -1 ≤ score ≤ 1 : RightTroll\"\"\"\n",
    "    \n",
    "    tot_hash=len(hashtags_collec)\n",
    "    hash_party= pd.DataFrame(np.zeros(tot_hash), columns={'Score'}, index=hashtags_collec)\n",
    "\n",
    "    hash_=[]\n",
    "    for tweet in df[(df.hashtags.isnull()==False) & (df.account_category=='RightTroll')].hashtags:\n",
    "        hash_+=tweet\n",
    "    count=pd.DataFrame.from_dict(Counter(hash_), columns={'R#'}, orient='index')\n",
    "\n",
    "    hash_party=hash_party.merge(count, how='left', right_index=True, left_index=True)\n",
    "\n",
    "    hash_=[]\n",
    "    for tweet in df[(df.hashtags.isnull()==False) & (df.account_category=='LeftTroll')].hashtags:\n",
    "        hash_+=tweet\n",
    "    count=pd.DataFrame.from_dict(Counter(hash_), columns={'L#'}, orient='index')\n",
    "\n",
    "    hash_party=hash_party.merge(count, how='left', right_index=True, left_index=True)\n",
    "    \n",
    "    hash_party.fillna(0,inplace=True)\n",
    "    hash_party['Score']=hash_party.apply(lambda row:  ((row['R#']/nb_right)-(row['L#']/nb_left))/((row['R#']/nb_right)+(row['L#']/nb_left)) if (row['R#']+row['L#'] >0)  else 0 , axis=1)\n",
    "\n",
    "    return hash_party\n",
    "\n",
    "def save_df(df, filename=pickle_files+'df.pkl'):\n",
    "    \n",
    "    with open(filename, 'wb') as d:\n",
    "        pickle.dump(df, d, pickle.HIGHEST_PROTOCOL)\n",
    "    print('--> DafaFrame saved as '+ filename)\n",
    "    \n",
    "def load_df(filename=(pickle_files+'df.pkl')):\n",
    "    with open(pickle_files+'df.pkl', 'rb') as d:\n",
    "        df=pickle.load(d)\n",
    "    print(' DafaFrame loaded <--')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exctract hastags that happend more than X (=10) time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_collec=get_hastag_collection(' '.join([tweet for tweet in df.content]), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to load the _df.pkl_ before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags']=df.content.apply(lambda text: hashtag_extractor(text,hashtags_collec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Hashtag political orientation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a color according to their score\n",
    "hashtags=hastag_orientation(df, hashtags_collec)\n",
    "hashtags['colors#'] = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.RdBu(mpl.colors.Normalize()(hashtags['Score'].apply(lambda x: -x).tolist()))\n",
    "]\n",
    "hashtags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe final step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start here to avoid the pre-process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading everything:\n",
    "def load_df(filename=(pickle_files+'df.pkl')):\n",
    "    with open(filename, 'rb') as d:\n",
    "        df=pickle.load(d)\n",
    "    print(' DafaFrame loaded <--')\n",
    "    return df\n",
    "\n",
    "def load_vocab(min_count=10):\n",
    "    \"\"\"Recovers the vocabulary from pickles\"\"\"\n",
    "    filename=pickle_files+'vocabulary_'+str(min_count)+'.pkl'\n",
    "\n",
    "    with open(filename, 'rb') as voc:\n",
    "        vocabulary=pickle.load(voc) \n",
    "    print(' Vocabulary loaded <--')\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c15b0da42564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_files\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'df_final.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'save_df' is not defined"
     ]
    }
   ],
   "source": [
    "save_df(df, pickle_files+'df_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DafaFrame loaded <--\n",
      " Vocabulary loaded <--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>content</th>\n",
       "      <th>account_category</th>\n",
       "      <th>Clean_tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[sit, democrat, senat, trial, corrupt, bare, p...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@nedryun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[marshawn, lynch, game, anti-trump, shirt, sag...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[fallen, navi, sailor, power, anthem, nfl, pac...</td>\n",
       "      <td>[#BoycottNFL]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[presid, trump, presid, tournament, texa, puer...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[anthem, anthem]</td>\n",
       "      <td>[#StandForOurAnthem]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publish_date                                            content  \\\n",
       "0   2017-10-01  \"We have a sitting Democrat US Senator on tria...   \n",
       "1   2017-10-01  Marshawn Lynch arrives to game in anti-Trump s...   \n",
       "2   2017-10-01  Daughter of fallen Navy Sailor delivers powerf...   \n",
       "3   2017-10-01  JUST IN: President Trump dedicates Presidents ...   \n",
       "4   2017-10-01  19,000 RESPECTING our National Anthem! #StandF...   \n",
       "\n",
       "  account_category                                        Clean_tweet  \\\n",
       "0       RightTroll  [sit, democrat, senat, trial, corrupt, bare, p...   \n",
       "1       RightTroll  [marshawn, lynch, game, anti-trump, shirt, sag...   \n",
       "2       RightTroll  [fallen, navi, sailor, power, anthem, nfl, pac...   \n",
       "3       RightTroll  [presid, trump, presid, tournament, texa, puer...   \n",
       "4       RightTroll                                   [anthem, anthem]   \n",
       "\n",
       "               hashtags   Mentioned  \n",
       "0                    []  [@nedryun]  \n",
       "1                    []          []  \n",
       "2         [#BoycottNFL]          []  \n",
       "3                    []          []  \n",
       "4  [#StandForOurAnthem]          []  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=load_df(pickle_files+'df_final.pkl')\n",
    "vocabulary=load_vocab()\n",
    "\n",
    "nb_left=df[df.account_category=='LeftTroll'].shape[0]\n",
    "nb_right=df[df.account_category=='RightTroll'].shape[0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet accounts mentioned @ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_extractor(content):\n",
    "    \"\"\"Extract @Names (the twitter account that is directly mentioned)\"\"\"\n",
    "    at = []\n",
    "    content=''.join(ch for ch in content if ch not in '!\"$%&\\()*+,./:;<=>?#[\\\\]^{|}~\\'').split(' ') \n",
    "    \n",
    "    for word in content:\n",
    "        if (word.startswith('@')) & (len(word)>1):  \n",
    "              at.append(word.encode('ascii',errors='ignore').decode())\n",
    "        \n",
    "        #tweet that are quoted\n",
    "        elif(word.startswith('\\'@') | word.startswith('\\\"@')) & (len(word)>1):\n",
    "            at.append(word[1:].encode('ascii',errors='ignore').decode())\n",
    "\n",
    "    return at\n",
    "\n",
    "def at_orientation(at_left, at_right):\n",
    "    \"\"\"Defines if a @word have been used more by Right or Left trolls.\n",
    "    LeftTroll: -1 ≤ score ≤ 1 : RightTroll\"\"\"\n",
    "        \n",
    "    #right\n",
    "    count_r=pd.DataFrame.from_dict(Counter(at_right), columns={'R@'}, orient='index')\n",
    "    \n",
    "    #left\n",
    "    count_l=pd.DataFrame.from_dict(Counter(at_left), columns={'L@'}, orient='index')\n",
    "\n",
    "    #all\n",
    "    at_oritation=count_r.merge(count_l, how='left', right_index=True, left_index=True)\n",
    "    \n",
    "    at_oritation.fillna(0,inplace=True)\n",
    "    at_oritation['Score']=at_oritation.apply(lambda row:  ((row['R@']/nb_right)-(row['L@']/nb_left))/((row['R@']/nb_right)+(row['L@']/nb_left)) if (row['R@']+row['L@'] >0)  else 0 , axis=1)\n",
    "\n",
    "    return at_oritation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mentioned']=df.content.apply(lambda tweet: at_extractor(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_left=sum(df[(df.account_category=='LeftTroll')].Mentioned.tolist(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_right=sum(df[(df.account_category=='RightTroll')].Mentioned.tolist(), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_all=at_orientation(at_left, at_right)\n",
    "at_all['colors@'] = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.RdBu(mpl.colors.Normalize()(at_all['Score'].apply(lambda x: -x).tolist()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word political orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word gets characterize by a *Score* that should represent it category: <br>\n",
    "        \n",
    " * LeftTroll: -1 ≤ score ≤ 1 : RightTroll\n",
    "        \n",
    "To get rid of neutral words, we take only words that are characteristic of one the two groups : \n",
    "\n",
    "* |Score| ≥ 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_orientation(df, vocab):\n",
    "    \"\"\"Defines if a words have been used more by Right or Left trolls.\n",
    "    LeftTroll: -1 ≤ score ≤ 1 : RightTroll\"\"\"\n",
    "    \n",
    "    tot_words=len(vocab)\n",
    "    words_party= pd.DataFrame(np.zeros(tot_words), columns={'Score'},index=vocab) #vocabulary dataframe\n",
    "\n",
    "    #Right trolls words\n",
    "    voc=[]\n",
    "    for tweet in df[(df.Clean_tweet.isnull()==False) & (df.account_category=='RightTroll')].Clean_tweet:\n",
    "        voc+=tweet\n",
    "    count=pd.DataFrame.from_dict(Counter(voc), columns={'R'}, orient='index') #get word frequency\n",
    "    words_party=words_party.merge(count, how='left', right_index=True, left_index=True) #add it to dataframe\n",
    "\n",
    "    #Left trolls words\n",
    "    voc=[]\n",
    "    for tweet in df[(df.Clean_tweet.isnull()==False) & (df.account_category=='LeftTroll')].Clean_tweet:\n",
    "        voc+=tweet\n",
    "    count=pd.DataFrame.from_dict(Counter(voc), columns={'L'}, orient='index')\n",
    "\n",
    "    words_party=words_party.merge(count, how='left', right_index=True, left_index=True)\n",
    "    words_party.fillna(0,inplace=True)# replace Nans\n",
    "    \n",
    "    #Computes the score of each word\n",
    "    words_party['Score']=words_party.apply(lambda row: ((row['R']/nb_right)-(row['L']/nb_left))/((row['R']/nb_right)+(row['L']/nb_left)), axis=1)\n",
    "\n",
    "    return words_party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove words that are not politically oriented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantifies de correlation between each category (right or left)\n",
    "vocabulary_oriented=word_orientation(df,vocabulary )\n",
    "\n",
    "#take words that are oriented. Remove neutral words : |orientation| < 0.25\n",
    "vocabulary_oriented=vocabulary_oriented[np.abs(vocabulary_oriented.Score) >= 0.25] \n",
    "print('The oriented vocabulary is {} long (before {} long.)'.format(len(vocabulary_oriented), len(vocabulary)))\n",
    "\n",
    "#remove words that are not kept in the vocabulary oriented\n",
    "df['Clean_tweet']=df.Clean_tweet.apply(lambda list_word: \\\n",
    "                                       [word for word in list_word if word in vocabulary_oriented.index]\\\n",
    "                                        if list_word else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non empty clean tweet\n",
    "embedding=100\n",
    "tweets=df[df.Clean_tweet.isnull()==False]['Clean_tweet'] #take non empty clean tweet\n",
    "model = Word2Vec(tweets.tolist(), min_count=1, size=embedding)\n",
    "model.train(tweets.tolist(), epochs=10, total_examples=model.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two following methodes were tried to have a better representations of the words (after the PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vectors):\n",
    "    \"\"\"Normalize the word vectors\"\"\"\n",
    "    norm=np.linalg.norm(model.wv.vectors, ord=2, axis=1)\n",
    "    vect_norm=np.zeros(vectors.shape)\n",
    "    for i in range(vectors.shape[0]):\n",
    "        vect_norm[i]=(vectors[i]/norm[i])\n",
    "    return vect_norm\n",
    "\n",
    "def standardize(vectors):\n",
    "    \"\"\"Standardize the word vectors\"\"\"\n",
    "    return (vectors-np.mean(vectors,axis=0))/np.std(vectors,axis=0)\n",
    "\n",
    "#norm_vectors=normalize(model.wv.vectors)\n",
    "sd_vectors=standardize(model.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfom a PCA on the model's vectors for plotting. Each word gets a color according to its orientation score (defiend above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = pd.DataFrame(model.wv.vectors, index=model.wv.vocab) #for tweet vector\n",
    "\n",
    "# fit a PCA and tke the first 9 PCs\n",
    "Nb_Pcs=9\n",
    "pca = PCA(n_components=Nb_Pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(pca.fit_transform(sd_vectors))#, columns={'X','Y','Z'})\n",
    "result.columns=['Pc'+str(i) for i in range(1,Nb_Pcs+1)]\n",
    "result.index=model.wv.vocab\n",
    "\n",
    "\n",
    "#defines a color according to their score\n",
    "result=result.merge(vocabulary_oriented, how='right', right_index=True, left_index=True)\n",
    "result['colors'] = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g, b, _ in 255*mpl.cm.RdBu(mpl.colors.Normalize()(result['Score'].apply(lambda x: -x).tolist()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_ *100 # % of variance explained by each componened --> very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.explained_variance_ratio_ *100) #total variance explained by all the considered PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total variance eplained by the 9 PCs is relatively low. The visualization on 2 dimensions will not be neat as more than half of the data variability will not be represented. Even if two points (representing words) will appear close to each other in 2D, they might be realy distant in the total 100 dimensions space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head() #data frame quick look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Plotting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Word space (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ---- X vs Y ----\n",
    "pXY = figure(plot_width=900, plot_height=450) # x_axis_type='datetime')\n",
    "pXY.title.text = 'Tweets topic clusters'\n",
    "\n",
    "right_word=result[result['Score'] >= 0.5 ]\n",
    "left_word=result[result['Score']<=-0.5]\n",
    "\n",
    "source_R = ColumnDataSource(data=right_word)\n",
    "pXY.scatter(x='Pc1', y='Pc2', source=source_R, fill_color='colors', legend='Right', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "source_L = ColumnDataSource(data=left_word)\n",
    "pXY.scatter(x='Pc1', y='Pc2', source=source_L, fill_color='colors',legend='Left', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "\n",
    "pXY.legend.location = 'top_left'\n",
    "pXY.legend.click_policy='hide'\n",
    "pXY.xaxis.axis_label='PC1'\n",
    "pXY.yaxis.axis_label='PC2'\n",
    "\n",
    "hover_tool=tools.HoverTool(\n",
    "    tooltips=[('Word', '@index')],\n",
    "    formatters={'index' : 'printf', },\n",
    "    mode='mouse'\n",
    ")\n",
    "\n",
    "pXY.tools.append(hover_tool)\n",
    "\n",
    "# ---- X vs Z ----\n",
    "pXZ = figure(plot_width=450, plot_height=450) # x_axis_type='datetime')\n",
    "pXZ.title.text = 'Tweets topic clusters'\n",
    "\n",
    "\n",
    "source_R = ColumnDataSource(data=right_word)\n",
    "pXZ.scatter(x='Pc1', y='Pc3', source=source_R, fill_color='colors', legend='Right', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "source_L = ColumnDataSource(data=left_word)\n",
    "pXZ.scatter(x='Pc1', y='Pc3', source=source_L, fill_color='colors',legend='Left', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "\n",
    "pXZ.legend.location = 'top_left'\n",
    "pXZ.legend.click_policy='hide'\n",
    "pXZ.xaxis.axis_label='PC1'\n",
    "pXZ.yaxis.axis_label='PC3'\n",
    "\n",
    "# --- Y vs Z ----\n",
    "pYZ = figure(plot_width=450, plot_height=450) # x_axis_type='datetime')\n",
    "pYZ.title.text = 'Tweets topic clusters'\n",
    "\n",
    "\n",
    "\n",
    "source_R = ColumnDataSource(data=right_word)\n",
    "pYZ.scatter(x='Pc2', y='Pc3', source=source_R, fill_color='colors', legend='Right', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "source_L = ColumnDataSource(data=left_word)\n",
    "pYZ.scatter(x='Pc2', y='Pc3', source=source_L, fill_color='colors',legend='Left', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "\n",
    "pYZ.legend.location = 'top_left'\n",
    "pYZ.legend.click_policy='hide'\n",
    "pYZ.xaxis.axis_label='PC2'\n",
    "pYZ.yaxis.axis_label='PC3'\n",
    "\n",
    "hover_tool=tools.HoverTool(\n",
    "    tooltips=[('Word', '@index')],\n",
    "    formatters={'index' : 'printf', },\n",
    "    mode='mouse'\n",
    ")\n",
    "\n",
    "pYZ.tools.append(hover_tool)\n",
    "pXZ.tools.append(hover_tool)\n",
    "\n",
    "output_file('Plots/Word2Vec_word_space.html')\n",
    "save(gridplot([[pXY],[ pXZ, pYZ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Plot is [here](Plots/Word2Vec_word_space.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plotting every combinaison of PCs together to try to find a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax=scatter_matrix(result.drop(columns={'Score','R', 'L', 'colors'}), figsize=[18,18], marker='.', c=result.colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We cannot see any cluster. The 2D reduction of 100D is not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tweets representation in Word2Vec space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Represent each tweet with K components (the number of word embedding dimensions) as the mean of each word's compenents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_vec(df,embedding, load=False):\n",
    "    \n",
    "    filename=pickle_files+'tweets_vec_'+str(embedding)+'.pkl'\n",
    "    \n",
    "    if load:\n",
    "        try:\n",
    "            with open(filename, 'rb') as tv:\n",
    "                tweets_vec=pickle.load(tv)\n",
    "            print('Tweet_vec loaded <--')\n",
    "\n",
    "        except:\n",
    "            print('Pikle not found')\n",
    "            tweets_vec= np.zeros((df.shape[0],embedding)) # nb_words x K \n",
    "            zero_vec=np.zeros(embedding)\n",
    "\n",
    "            for  idx, words in enumerate(df.Clean_tweet):\n",
    "                if words:  #if not None   \n",
    "                    tweets_vec[idx,:]=np.mean(word_vec.loc[words,:], axis=0) # mean over words-vectors (the weighting is included)\n",
    "                else:\n",
    "                    tweets_vec[idx,:]=zero_vec\n",
    "\n",
    "            with open(filename, 'wb') as tv:\n",
    "                pickle.dump(tweets_vec, tv, pickle.HIGHEST_PROTOCOL) #saving the tweet_vect\n",
    "            print('Tweet_vec saved -->')\n",
    "                \n",
    "    else:\n",
    "        tweets_vec= np.zeros((df.shape[0],embedding)) # nb_words x K \n",
    "        zero_vec=np.zeros(embedding)\n",
    "\n",
    "        for  idx, words in enumerate(df.Clean_tweet):\n",
    "            if words:  #if not None   \n",
    "                tweets_vec[idx,:]=np.mean(word_vec.loc[words,:], axis=0) # mean over words-vectors (the weighting is included)\n",
    "            else:\n",
    "                tweets_vec[idx,:]=zero_vec\n",
    "\n",
    "        with open(filename, 'wb') as tv:\n",
    "            pickle.dump(tweets_vec, tv, pickle.HIGHEST_PROTOCOL) #saving the tweet_vect\n",
    "        print('Tweet_vec saved -->')\n",
    "        \n",
    "    return tweets_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweets_vec=get_tweet_vec(df, embedding, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweets_vec=standardize(tweets_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "tweet_vec_pca = pd.DataFrame(pca.fit_transform(model.wv.vectors),columns={'X','Y','Z'})\n",
    "tweet_vec_pca['category']=df.account_category\n",
    "tweet_vec_pca['colors']=tweet_vec_pca.apply(lambda row: '#ff6347' if row['category']=='RightTroll' else '#1E90FF', axis=1)\n",
    "tweet_vec_pca['tweet']=df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweet_vec_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ---- PC1 vs PC2 ---- #\n",
    "pXY = figure(plot_width=500, plot_height=450)\n",
    "pXY.title.text = 'Tweets topic clusters'\n",
    "\n",
    "\n",
    "source_pca_R = ColumnDataSource(data=tweet_vec_pca[tweet_vec_pca.category== 'RightTroll'])\n",
    "pXY.scatter(x='X', y='Y', source=source_pca_R, fill_color='colors', legend='RightTroll', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "source_pca_L = ColumnDataSource(data=tweet_vec_pca[tweet_vec_pca.category== 'LeftTroll'])\n",
    "pXY.scatter(x='X', y='Y', source=source_pca_L, fill_color='colors', legend='LeftTroll', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "\n",
    "pXY.legend.location = 'top_left'\n",
    "pXY.legend.click_policy='hide'\n",
    "pXY.xaxis.axis_label='PC1'\n",
    "pXY.yaxis.axis_label='PC2'\n",
    "\n",
    "hover_tool=tools.HoverTool(\n",
    "    tooltips=[('Tweet', '@tweet')],\n",
    "    formatters={'tweet' : 'printf', },\n",
    "    mode='mouse'\n",
    ")\n",
    "\n",
    "pXY.tools.append(hover_tool)\n",
    "\n",
    "# ---- PC1 vs PC3 ---- #\n",
    "pXZ = figure(plot_width=500, plot_height=450)\n",
    "pXZ.title.text = 'Tweets topic clusters'\n",
    "\n",
    "\n",
    "source_pca_R = ColumnDataSource(data=tweet_vec_pca[tweet_vec_pca.category== 'RightTroll'])\n",
    "pXZ.scatter(x='X', y='Z', source=source_pca_R, fill_color='colors', legend='RightTroll', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "source_pca_L = ColumnDataSource(data=tweet_vec_pca[tweet_vec_pca.category== 'LeftTroll'])\n",
    "pXZ.scatter(x='X', y='Z', source=source_pca_L, fill_color='colors', legend='LeftTroll', fill_alpha=0.6, line_color=None)\n",
    "\n",
    "\n",
    "pXZ.legend.location = 'top_left'\n",
    "pXZ.legend.click_policy='hide'\n",
    "pXZ.xaxis.axis_label='PC1'\n",
    "pXZ.yaxis.axis_label='PC3'\n",
    "\n",
    "\n",
    "pXZ.tools.append(hover_tool)\n",
    "\n",
    "output_file('Plots/Word2Vec_Tweets.html')\n",
    "save(row(pXY, pXZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plot can be seen [here](Plots/Word2Vec_Tweets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax=scatter_matrix(tweet_vec_pca.drop(columns={'tweet', 'category'}), figsize=[10,10], marker='.', c=tweet_vec_pca.colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequent words & hashtags per category "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words, #hashtags and @mentioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Use of POTUS (president of the US) twitter account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Trump presidency began on the 20th of January 2017\n",
    "trump_presidency = df[df.publish_date > datetime.strptime('2017-20-01', '%Y-%d-%m')]\n",
    "obama_presidency = df[df.publish_date < datetime.strptime('2017-20-01', '%Y-%d-%m')]\n",
    "\n",
    "def POTUS_count():\n",
    "    '''\n",
    "    Counts the number of appearance of @POTUS for trump and obama presidency by account_category\n",
    "    '''\n",
    "    potus_counter = Counter({'Trump-RightTroll': 0, 'Trump-LeftTroll': 0, 'Obama-RightTroll': 0, 'Obama-LeftTroll': 0})\n",
    "    for row in range(len(obama_presidency)):\n",
    "        if len(obama_presidency.iloc[row].Mentioned) > 0:\n",
    "            for mention in obama_presidency.iloc[row].Mentioned:\n",
    "                if mention == '@POTUS':\n",
    "                    account_category = obama_presidency.iloc[row].account_category\n",
    "                    potus_counter['Obama-'+account_category] += 1\n",
    "    for row in range(len(trump_presidency)):\n",
    "        if len(trump_presidency.iloc[row].Mentioned) > 0:\n",
    "            for mention in trump_presidency.iloc[row].Mentioned:\n",
    "                if mention == '@POTUS':\n",
    "                    account_category = trump_presidency.iloc[row].account_category\n",
    "                    potus_counter['Trump-'+account_category] += 1\n",
    "    return potus_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "potus_counter = POTUS_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Displaying results as a confusion matrix\n",
    "tot = sum(potus_counter.values())\n",
    "trump_left_potus = round((100*potus_counter['Trump-LeftTroll'])/tot,2)\n",
    "trump_right_potus = round((100*potus_counter['Trump-RightTroll'])/tot,2)\n",
    "obama_left_potus = round((100*potus_counter['Obama-LeftTroll'])/tot,2)\n",
    "obama_right_potus = round((100*potus_counter['Obama-RightTroll'])/tot,2)\n",
    "potus_array = pd.DataFrame(data = {'Trump': [trump_right_potus, trump_left_potus, trump_right_potus+trump_left_potus], \\\n",
    "                                   'Obama': [obama_right_potus, obama_left_potus, obama_right_potus+obama_left_potus], \\\n",
    "                                   'Category Sum': [trump_right_potus+obama_right_potus, trump_left_potus+obama_left_potus, 100]},\\\n",
    "                          index = [\"Right Trolls\", \"Left Trolls\", \"President Sum\"])\n",
    "potus_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plottings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Def of at and hash\n",
    "top_people_right = ['@realDonaldTrump', '@POTUS', '@HillaryClinton', '@CNN', '@FoxNews', '@YouTube', '@CNNPolitics', \\\n",
    "             '@seanhannity', '@BreitbartNews', '@mashable']\n",
    "\n",
    "top_people_left = ['@YouTube', '@TalibKweli', '@realDonaldTrump', '@josephjett', '@POTUS', '@deray',\\\n",
    "                   '@docrocktex26', '@ShaunKing', '@HillaryClinton', '@BlackGirlNerds']\n",
    "\n",
    "top_people_expl_right = [\"Donald Trump's personal account\", 'President Of The United State twitter account; used by both Trump (66%) and Obama', \\\n",
    "                \"Hillary Clinton's personal account\", \"CNN news official account\", \"FoxNews official account\", \\\n",
    "                \"YouTube official account\", \"CNN political twitter channel\", \\\n",
    "                \"Sean Hannity is an Fow News talk show host and conservative political commentator\", \\\n",
    "                \"Breitbart News is a far-right American news, created by Steve Bannon ex-White House Chief Strategist\", \\\n",
    "                 \"Mashable is a digital media website\",]\n",
    "\n",
    "top_people_expl_left = [\"YouTube official account\", \"Talib Kweli is a hip hop recording artist and social activist\", \\\n",
    "                        \"Donald Trump's personal account\", \\\n",
    "                         \"Joseph Jett is a former securities trader, known for his role in the Kidder Peabody trading loss in 1994, at the time the largest trading fraud in history\", \\\n",
    "                        'President Of The United State twitter account; used by both Trump (66%) and Obama',\\\n",
    "                        \"DeRay Mckesson is a civil rights activist supporter of the Black Lives Matter movement\", \\\n",
    "                         \"She is a black democrat activist\", \\\n",
    "                        \"Shaun King is a writer and civil rights activist, supporting the Black Lives Matter movement\", \\\n",
    "                        \"Hillary Clinton's personal account\",\\\n",
    "                        \"Black Girl Nerds is an inclusive website promoting diversity\"]\n",
    "\n",
    "\n",
    "d = {'People': top_people_right, 'Explanation': top_people_expl_right}\n",
    "people_explanation_right = pd.DataFrame(data=d)\n",
    "\n",
    "d = {'People': top_people_left, 'Explanation': top_people_expl_left}\n",
    "people_explanation_left = pd.DataFrame(data=d)\n",
    "\n",
    "top_hashtag_right = ['#MAGA', '#tcot', \"#PJNET\", '#top', '#news', '#mar', '#topl', '#IslamKills', '#2A', '#WakeUpAmerica']\n",
    "\n",
    "top_hashtag_left = [\"#BlackLivesMatter\", \"#NowPlaying\", \"#BlackTwitter\", \"#news\", \"#PoliceBrutality\", \"#blacklivesmatter\",\\\n",
    "              \"#StayWoke\", \"#BlackSkinIsNotACrime\", \"#BLM\", \"#hiphop\"]\n",
    "\n",
    "top_hashtag_expl_right = [\"Donald Trump's slogan: Make America Great Again\", \"Top Conservatives on Twitter\", \n",
    "                   \"The Patriot Journalist Network, a news network known for bombarding Twitter with thousands of posts per day about political debates\", \\\n",
    "                   'to define', \"Relative to news\", \"Stands for Mar-A-Lago, Trump's palm beach golf course\", \n",
    "                   'To define', 'Hastag used in reaction to islamic terror attacks', \n",
    "                   \"Stands for the Second Amendment of the US Constitution: The right to keep and bear arms\", \\\n",
    "                   \"To define\"]\n",
    "\n",
    "top_hashtag_expl_left = [\"Black Lives Matter is an activist movement that campaigns against violence and systemic racism towards black people\", \\\n",
    "                   \"Hashtag used to share music\", \\\n",
    "                    \"Black Twitter is a cultural identity of Black Twitter users focused on issues of interest to the black community\", \\\n",
    "                   \"Relative to news\", \"Refers to Police brutality against the black community\", \\\n",
    "                   \"Black Lives Matter is an activist movement that campaigns against violence and systemic racism towards black people\", \\\n",
    "                   \"To stay woke is to keep informed of what going on pecifically on occasions when the media is being heavily filtered\", \\\n",
    "                   \"Refers police discrimination against african-americans\", \n",
    "                   \"Short for Black Lives Matter\", \"Refers to Hip-Hop music\"]\n",
    "\n",
    "d = {'Hashtags': top_hashtag_right, 'Explanation': top_hashtag_expl_right}\n",
    "hashtag_explanation_right = pd.DataFrame(data=d)\n",
    "d = {'Hashtags': top_hashtag_left, 'Explanation': top_hashtag_expl_left}\n",
    "hashtag_explanation_left = pd.DataFrame(data=d)\n",
    "\n",
    "#Getting the most popular words and hastags per category and displaying the appearance percentage per category:\n",
    "top=10\n",
    "popular_right=result.sort_values(by='R',ascending=False)[:top].sort_values(by='R')\n",
    "popular_right['Percentage'] = (100*popular_right['R'])/len(df[df.account_category=='RightTroll'])\n",
    "popular_left=result.sort_values(by='L',ascending=False)[:top].sort_values(by='L')\n",
    "popular_left['Percentage'] = (100*popular_left['L'])/len(df[df.account_category=='LeftTroll'])\n",
    "\n",
    "popular_right_h=hashtags.sort_values(by='R#',ascending=False)[:top].sort_values(by='R#')\n",
    "popular_right_h = popular_right_h.join(hashtag_explanation_right.set_index('Hashtags'))\n",
    "popular_right_h['Percentage'] = (100*popular_right_h['R#'])/len(df[df.account_category=='RightTroll'])\n",
    "popular_left_h=hashtags.sort_values(by='L#',ascending=False)[:top].sort_values(by='L#')\n",
    "popular_left_h = popular_left_h.join(hashtag_explanation_left.set_index('Hashtags'))\n",
    "popular_left_h['Percentage'] = (100*popular_left_h['L#'])/len(df[df.account_category=='LeftTroll'])\n",
    "\n",
    "\n",
    "\n",
    "popular_right_at=at_all.sort_values(by='R@',ascending=False)[:top].sort_values(by='R@')\n",
    "popular_right_at = popular_right_at.join(people_explanation_right.set_index('People'))\n",
    "popular_right_at['Percentage'] = (100*popular_right_at['R@'])/len(df[df.account_category=='RightTroll'])\n",
    "popular_left_at=at_all.sort_values(by='L@',ascending=False)[:top].sort_values(by='L@')\n",
    "popular_left_at = popular_left_at.join(people_explanation_left.set_index('People'))\n",
    "popular_left_at['Percentage'] = (100*popular_left_at['L@'])/len(df[df.account_category=='LeftTroll'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "#Right trolls:\n",
    "x_range=(0, popular_right['R'].max() + 1000)\n",
    "sourceR=ColumnDataSource(data=popular_right)\n",
    "pR = figure(plot_width=450, plot_height=350, y_range=popular_right.index.tolist(), toolbar_location=None, x_range=x_range)\n",
    "pR.hbar(right='R', y='index', height=0.95, color='colors', source=sourceR)\n",
    "pR.title.text = 'Popular words for RightTrolls'\n",
    "pR.xaxis.axis_label= 'count'\n",
    "\n",
    "x_range_h=(0, popular_right_h['R#'].max())\n",
    "sourceR_hash=ColumnDataSource(data=popular_right_h)\n",
    "pR_hash = figure(plot_width=450, plot_height=350, y_range=popular_right_h.index.tolist(), toolbar_location=None, x_range=x_range_h)\n",
    "pR_hash.hbar(right='R#', y='index', height=0.95, color='colors#', source=sourceR_hash)\n",
    "pR_hash.title.text = 'Popular Hastags for RightTrolls'\n",
    "pR_hash.xaxis.axis_label= 'count'\n",
    "\n",
    "x_range_at=(0, popular_right_at['R@'].max())\n",
    "sourceR_at=ColumnDataSource(data=popular_right_at)\n",
    "pR_at = figure(plot_width=450, plot_height=350, y_range=popular_right_at.index.tolist(), toolbar_location=None, x_range=x_range_at)\n",
    "pR_at.hbar(right='R@', y='index', height=0.95, color='colors@', source=sourceR_at)\n",
    "pR_at.title.text = 'Popular Mentioned Accounts for RightTrolls'\n",
    "pR_at.xaxis.axis_label= 'count'\n",
    "\n",
    "\n",
    "\n",
    "#Left trolls:\n",
    "\n",
    "sourceL=ColumnDataSource(data=popular_left)\n",
    "pL = figure(plot_width=450, plot_height=350, y_range=popular_left.index.tolist(), toolbar_location=None, x_range=x_range) \n",
    "pL.hbar(right='L', y='index', height=0.95, color='colors', source=sourceL, )\n",
    "pL.title.text = 'Popular words for LeftTrolls'\n",
    "pL.xaxis.axis_label= 'count'\n",
    "\n",
    "\n",
    "\n",
    "sourceL_hash=ColumnDataSource(data=popular_left_h)\n",
    "pL_hash = figure(plot_width=450, plot_height=350, y_range=popular_left_h.index.tolist(), toolbar_location=None, x_range=x_range_h)\n",
    "pL_hash.hbar(right='L#', y='index', height=0.95, color='colors#', source=sourceL_hash)\n",
    "pL_hash.title.text = 'Popular Hastags for LeftTrolls'\n",
    "pL_hash.xaxis.axis_label= 'count'\n",
    "\n",
    "\n",
    "\n",
    "sourceL_at=ColumnDataSource(data=popular_left_at)\n",
    "pL_at = figure(plot_width=450, plot_height=350, y_range=popular_left_at.index.tolist(), toolbar_location=None, x_range=x_range_at)\n",
    "pL_at.hbar(right='L@', y='index', height=0.95, color='colors@', source=sourceL_at)\n",
    "pL_at.title.text = 'Popular Mentioned Accounts for LeftTrolls'\n",
    "pL_at.xaxis.axis_label= 'count'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#hover settings\n",
    "hover_tool_expl_at=tools.HoverTool(\n",
    "    tooltips=[('Orientation', '@Score'), \n",
    "             ('Explanation', '@Explanation'), \n",
    "             ('Category', '@Percentage %')],\n",
    "    formatters={'Score' : 'printf', \n",
    "               'Explanation' : 'printf',\n",
    "               'Percentage' : 'printf'},\n",
    "    mode='mouse', \n",
    "    #attachment = 'right'\n",
    ")\n",
    "\n",
    "#hover settings\n",
    "hover_tool_expl_h=tools.HoverTool(\n",
    "    tooltips=\"\"\"\n",
    "    <div style=\"max-width: 300px;\">\n",
    "<div><span style=\"color: #26aae1;\">Orientation</span>: @Score</div>\n",
    "<div><span style=\"color: #26aae1;\">Expalantion</span>:@Explanation</div>\n",
    " <div><span style=\"color: #26aae1;\">Category</span>: @Percentage %</div>\n",
    "</div>\"\"\",\n",
    "    formatters={'Score' : 'printf', \n",
    "               'Explanation' : 'printf',\n",
    "               'Percentage' : 'printf'},\n",
    "    mode='mouse', \n",
    "    attachment = 'below'\n",
    ")\n",
    "\n",
    "hover_tool=tools.HoverTool(\n",
    "    tooltips=[('Orientation', '@Score'), \n",
    "             ('Category', '@Percentage %')],\n",
    "    formatters={'Score' : 'printf',\n",
    "               'Percentage' : 'printf'},\n",
    "    mode='mouse', \n",
    "    attachment = 'right'\n",
    ")\n",
    "\n",
    "\n",
    "pL.tools.append(hover_tool)\n",
    "pR.tools.append(hover_tool)\n",
    "pR_hash.tools.append(hover_tool_expl_h)\n",
    "pL_hash.tools.append(hover_tool_expl_h)\n",
    "pR_at.tools.append(hover_tool_expl_at)\n",
    "pL_at.tools.append(hover_tool_expl_at)\n",
    "\n",
    "pL.xaxis[0].formatter = PrintfTickFormatter(format=\"%4.1e\")\n",
    "pR.xaxis[0].formatter = PrintfTickFormatter(format=\"%4.1e\")\n",
    "#output_notebook()\n",
    "output_file(plot_files+'Top_Word_hash_at.html')\n",
    "save(gridplot([[pR,pL], [pR_hash, pL_hash], [pR_at, pL_at]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot can be seen [here](Plots/Top_Word_hash_at.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(plot_files+'Top_words.html')\n",
    "save(row([pL,pR]))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(plot_files+'Top_hashtags.html')\n",
    "save(row([pL_hash, pR_hash]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file(plot_files+'Top_at.html')\n",
    "save(row([pL_at, pR_at]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('Orientation', '@Score'), \n",
    "             ('Explanation', '@Explanation'), \n",
    "             ('Category', '@Percentage %')],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Top hashtag per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Getting sum of tweets by account_cat and publish_date\n",
    "df_plot = pd.DataFrame(df.groupby(by=[\"account_category\", \"publish_date\"]).content.agg(\"count\"))\n",
    "df_plot = df_plot.unstack(level=0)\n",
    "\n",
    "#Dataframe with the most popular hashtag and count\n",
    "df_hash = df[['publish_date', 'account_category', 'hashtags']].copy()\n",
    "df_hash.set_index([\"account_category\", \"publish_date\"], inplace=True)\n",
    "\n",
    "hash_per_day = (df_hash.hashtags.apply(pd.Series)\n",
    "              .stack()\n",
    "              .reset_index(level=2, drop=True)\n",
    "              .to_frame('hashtags'))\n",
    "hash_per_day['count'] = 1\n",
    "\n",
    "#Making every strong lower cap\n",
    "hash_per_day.hashtags = hash_per_day.hashtags.str.lower()\n",
    "#Counting the top hashtags per day\n",
    "top_hash_per_day = hash_per_day.groupby([\"account_category\", \"publish_date\", \"hashtags\"]).agg(\"count\")\n",
    "#Releasing hashtag from index\n",
    "top_hash_per_day = top_hash_per_day.reset_index(level=2)\n",
    "#selecting the max hashtags per day\n",
    "top_hash_per_day = top_hash_per_day.groupby(by=[\"account_category\", \"publish_date\"]).max()\n",
    "#Releasing account_cat from index\n",
    "top_hash_per_day = top_hash_per_day.unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#merging top hash per day and and sum of tweets\n",
    "final_plot = pd.merge(df_plot, top_hash_per_day, left_index=True, right_index=True)\n",
    "final_plot.rename(columns={\"content\": \"total_daily_tweets\", \"count\": \"top_hashtag_count\"}, inplace=True)\n",
    "#Separating left and righ\n",
    "df_plot_left = pd.DataFrame(data={'total_daily_tweets': final_plot[\"total_daily_tweets\"][\"LeftTroll\"],\\\n",
    "                   'top_hashtag': final_plot[\"hashtags\"][\"LeftTroll\"],\\\n",
    "                   'top_hashtag_count': final_plot[\"top_hashtag_count\"][\"LeftTroll\"]})\n",
    "df_plot_right = pd.DataFrame(data={'total_daily_tweets': final_plot[\"total_daily_tweets\"][\"RightTroll\"],\\\n",
    "                   'top_hashtag': final_plot[\"hashtags\"][\"RightTroll\"],\\\n",
    "                   'top_hashtag_count': final_plot[\"top_hashtag_count\"][\"RightTroll\"]})\n",
    "\n",
    "#filling nans with strings\n",
    "df_plot_left.total_daily_tweets.fillna(value=\"No LeftTroll tweet\", inplace=True)\n",
    "df_plot_left.top_hashtag.fillna(value=\"No LeftTroll hashtag\", inplace=True)\n",
    "df_plot_left.top_hashtag_count.fillna(value=\"No LeftTroll hashtag\", inplace=True)\n",
    "df_plot_right.total_daily_tweets.fillna(value=\"No RightTroll tweet\", inplace=True)\n",
    "df_plot_right.top_hashtag.fillna(value=\"No RightTroll hashtag\", inplace=True)\n",
    "df_plot_right.top_hashtag_count.fillna(value=\"No RightTroll hashtag\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#tweet plot\n",
    "p = figure(plot_width=950, plot_height=400, x_axis_type='datetime', toolbar_location=None)\n",
    "p.title.text = 'Tweet density'\n",
    "p.yaxis.axis_label = 'Number of tweets'\n",
    "p.xaxis.axis_label = 'Time'\n",
    "cat_color = [\"tomato\", \"dodgerblue\"]\n",
    "\n",
    "source = ColumnDataSource(data=df_plot_left)\n",
    "pleft = p.line(x='publish_date', y='total_daily_tweets', source=source,\\\n",
    "            line_width=2, alpha=0.8, legend=\"LeftTroll\", color=cat_color[1])\n",
    "\n",
    "hover_tool_left=tools.HoverTool(\n",
    "    tooltips=[\n",
    "        ('Date', '@publish_date{%b %d, %Y}'),\n",
    "        ('Number of tweets','@total_daily_tweets'),\n",
    "        ('Top trending hashtag','@top_hashtag'),\n",
    "        ('Top hashtag count','@top_hashtag_count')],\n",
    "\n",
    "    formatters={\n",
    "        'publish_date' : 'datetime', # use 'datetime' formatter for 'date' field\n",
    "        'total_daily_tweets' : 'printf', \n",
    "        'top_hashtag' : 'printf', \n",
    "        'top_hashtag_count' : 'printf'},\n",
    "\n",
    "    # display a tooltip whenever the cursor is vertically in line with a glyph\n",
    "    mode='vline', \n",
    "    attachment='below',\n",
    "    renderers = [pleft]\n",
    ")\n",
    "\n",
    "p.tools.append(hover_tool_left)\n",
    "\n",
    "\n",
    "source = ColumnDataSource(data=df_plot_right)\n",
    "pright = p.line(x='publish_date', y='total_daily_tweets', source=source,\\\n",
    "            line_width=2, alpha=0.8, legend=\"RightTroll\", color=cat_color[0])\n",
    "\n",
    "p.legend.location = 'top_left'\n",
    "p.legend.click_policy='hide'\n",
    "\n",
    "hover_tool=tools.HoverTool(\n",
    "    tooltips=[\n",
    "        ('Date', '@publish_date{%b %d, %Y}'),\n",
    "        ('Number of tweets','@total_daily_tweets'),\n",
    "        ('Top trending hashtag','@top_hashtag'),\n",
    "        ('Top hashtag count','@top_hashtag_count')],\n",
    "\n",
    "    formatters={\n",
    "        'publish_date' : 'datetime', # use 'datetime' formatter for 'date' field\n",
    "        'total_daily_tweets' : 'printf', \n",
    "        'top_hashtag' : 'printf', \n",
    "        'top_hashtag_count' : 'printf'},\n",
    "\n",
    "    # display a tooltip whenever the cursor is vertically in line with a glyph\n",
    "    mode='vline', \n",
    "    attachment='above',\n",
    "    renderers = [pright]\n",
    ")\n",
    "p.tools.append(hover_tool)\n",
    "\n",
    "#output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nicolasgandar/Desktop/EPFL/ADA/Projet/ADA-Proj-18/Plots/TopHashthag_perday.html'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show(p)\n",
    "output_file(plot_files+'TopHashthag_perday.html')\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The plot can be seen [here](Plots/TopHashtag_perday.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
