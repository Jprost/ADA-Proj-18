{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bokeh.models import *\n",
    "from bokeh.plotting import *\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.palettes import Category20\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet, brown, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/Jean-\n",
      "[nltk_data]     BaptistePROST/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='Data/russian-troll-tweets/IRAhandle_tweets_'\n",
    "pickle_files='Pickles/'\n",
    "WORD_FREQ = 'Data/wordfrea.xlsx'\n",
    "plot_files='Plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal of this notebook is to represent the tweet by topics (tahter than category). <br>\n",
    "\n",
    "By looking at the most popular hastag per day, we decided to define 13 arbitrary categories that seemed relevant to characterized the tweets. A first list of words was build for each topic (~ 7 words/topic), then the `Word2Vec` model was used to exten the list. The model enables to find the words that have a large (cosine) similarity the the its word space. The topic became 10 times larger. <br>\n",
    "\n",
    "The tweet activity (number of tweets per day) for each topic was ploted. Clear peaks of activity were observable. \n",
    "Are those peaks related to a specific event?\n",
    "\n",
    "\n",
    "To figure this out, we used [Wiki Portal](https://en.wikipedia.org/wiki/Portal:Current_events) to retrived the information about the event of a particular day. An automatic event dector was build. For every spike of each topic, we tryed to match an event description scrapped from Wiki Portal.<br>\n",
    "We were thus able to label some peak of the tweet activity for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Loading & Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#loading data build in Word2vec.ipynb\n",
    "def load_df(filename=(pickle_files+'df_final.pkl')):\n",
    "    with open(filename, 'rb') as d:\n",
    "        df=pickle.load(d)\n",
    "    print(' DafaFrame loaded <--')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DafaFrame loaded <--\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>content</th>\n",
       "      <th>account_category</th>\n",
       "      <th>Clean_tweet</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[sit, democrat, us, senat, trial, corrupt, bar...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[marshawn, lynch, arriv, game, anti-trump, shi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[daughter, fallen, navi, sailor, deliv, power,...</td>\n",
       "      <td>[#BoycottNFL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[presid, trump, dedic, presid, cup, golf, tour...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[respect, nation, anthem, stand, anthem]</td>\n",
       "      <td>[#StandForOurAnthem]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  publish_date                                            content  \\\n",
       "0   2017-10-01  \"We have a sitting Democrat US Senator on tria...   \n",
       "1   2017-10-01  Marshawn Lynch arrives to game in anti-Trump s...   \n",
       "2   2017-10-01  Daughter of fallen Navy Sailor delivers powerf...   \n",
       "3   2017-10-01  JUST IN: President Trump dedicates Presidents ...   \n",
       "4   2017-10-01  19,000 RESPECTING our National Anthem! #StandF...   \n",
       "\n",
       "  account_category                                        Clean_tweet  \\\n",
       "0       RightTroll  [sit, democrat, us, senat, trial, corrupt, bar...   \n",
       "1       RightTroll  [marshawn, lynch, arriv, game, anti-trump, shi...   \n",
       "2       RightTroll  [daughter, fallen, navi, sailor, deliv, power,...   \n",
       "3       RightTroll  [presid, trump, dedic, presid, cup, golf, tour...   \n",
       "4       RightTroll           [respect, nation, anthem, stand, anthem]   \n",
       "\n",
       "               hashtags  \n",
       "0                    []  \n",
       "1                    []  \n",
       "2         [#BoycottNFL]  \n",
       "3                    []  \n",
       "4  [#StandForOurAnthem]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=load_df()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Word Freq* is a online dictionary (wordfrequency.info) that gives the occurency the 5000 most commun english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>PoS</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>the</td>\n",
       "      <td>a</td>\n",
       "      <td>0.066825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>be</td>\n",
       "      <td>v</td>\n",
       "      <td>0.038041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>and</td>\n",
       "      <td>c</td>\n",
       "      <td>0.032569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>of</td>\n",
       "      <td>i</td>\n",
       "      <td>0.031365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>0.030759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word PoS  Frequency\n",
       "Rank                    \n",
       "1.0   the   a   0.066825\n",
       "2.0    be   v   0.038041\n",
       "3.0   and   c   0.032569\n",
       "4.0    of   i   0.031365\n",
       "5.0     a   a   0.030759"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data frequency from http://www.wordfrequency.info\n",
    "wordfrequency = pd.read_excel(WORD_FREQ, header=0, index_col=[0], usecols=3).dropna()\n",
    "wordfrequency.Frequency = wordfrequency.Frequency/(wordfrequency.Frequency.sum())\n",
    "wordfrequency.rename(columns={'\\xa0\\xa0\\xa0Word' : 'Word', 'Part of speech' : 'PoS'}, inplace=True)\n",
    "wordfrequency.Word = wordfrequency.apply(lambda row: row['Word'].replace(\"\\xa0\\xa0\\xa0\", ''), axis=1)\n",
    "\n",
    "#5 most communs words\n",
    "wordfrequency.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Word2Vec model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=29316, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "embedding=100\n",
    "#non empty clean tweet\n",
    "tweets=df[df.Clean_tweet.isnull()==False]['Clean_tweet'] #take non empty clean tweet\n",
    "\n",
    "model = Word2Vec(tweets.tolist(), min_count=1, size=embedding)\n",
    "vocab_model=model.wv.vocab\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identify a list of topic that are frequent and were a debat at the time. We will expore their behavior in the word space hopping to find clusters :\n",
    "* Black lives matter, police brutality, police violence, blacktwitter, racism, NFL Protest, Jamar Clark, Alfredo Olango \n",
    "* music, thefourhorsemen, album \n",
    "* Trump, Donald\n",
    "* Hillary, HeforShe, IamWithHer, crookedHillary \n",
    "* religion\n",
    "* fear, North Korea, Russia, Geopolotics\n",
    "* voter fraud\n",
    "* terror, terrorist, attack, chicago, shootings, baltimore, bombings, Chattanooga\n",
    "* hacking, emails, DNC\n",
    "* Money, scandal, Wells Fargo, Imran Awan\n",
    "* election, campaign, GOP, DNC, Dem, vote, I voted, debate, primary, national convention\n",
    "* alt-righ, alt-left, Charlottesville, neo nazi\n",
    "* economy, deal, Nafta\n",
    "* music, thefourhorsemen, album, fm, nowplaying\n",
    "\n",
    "\n",
    "The list will get expanded thanks to the Word2Vec model. The words that hace a (cosine) similarity larger than a thrshold (0.6) are kept and appended to the list of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_raw=[['Music', 'lyrics', 'album','musician', 'nowplaying', 'soundcloud','rap', 'rnb','hip-hop', 'eminem','nas', 'horsemen', 'dj', 'wayne', 'snoop'],\\\n",
    "        ['Trump', 'melania', 'Donald', 'trump2016','realdonaldtrump', 'anti-trump', 'maga', 'MakeAmericaGreatAgain'],\\\n",
    "        ['Hillary', 'hilliari',  'HeforShe', 'IamWithHer', 'crookedHillary', 'clinton','killari','neverhillari', ],\\\n",
    "        ['Religion', 'islam', 'christianism', 'judaism'],\\\n",
    "        ['Fear', 'Korea', 'North', 'NorthKorea', 'Russia', 'putin', 'Geopolotics', 'kim', 'jong', 'kimjongun'],\\\n",
    "        ['BlackLivesMatters','BlackLiveMatter', 'black', 'trayvonmartin', 'mikebrown', 'sandrabland', 'bland', 'kaepernick','policeviolence', 'brutality' , 'fuckthepolice', 'cop', \\\n",
    "         'racial', 'blacktwitter', 'colin', 'kaepernick','racism' ],\\\n",
    "        ['Voter', 'fraud'],\\\n",
    "        ['MeToo', 'Weinstein', 'sexual', 'harassment', 'predator', 'feminist'],\\\n",
    "        ['Hurricane', 'Irma', 'matthew', 'flood', 'thunderstorm' ],\\\n",
    "        ['Terrorism', 'terror' 'terrorist', 'parisattack', 'londonattack', 'shootings', 'baltimore', 'bombings', 'Chattanooga', 'prayforbrussel', 'brussel', 'massacre'],\\\n",
    "        ['Hacking', 'emails', 'DNC'],\\\n",
    "        ['Obama', 'barack', 'michelle'],\\\n",
    "        ['Money', 'scandal', 'WellsFargo', 'Fargo', 'ImranAwan', 'Imran', 'Awan', 'Wasserman'],\\\n",
    "        ['Election', 'campaign', 'GOP','gopdebate', 'DNC', 'Dem', 'demdebate', 'vote', 'Ivoted', 'debate', 'primary', 'convention', 'america'],\\\n",
    "        ['Charlottesville','alt-righ', 'alt-left',  'neonazi', 'neo-nazi','kkk', 'klan', 'supremacist', 'unitetheright'], \\\n",
    "        ['Economy', 'deal', 'Nafta','stock', 'market','bitcoin', 'nasdaq']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hrc', 0.6785647869110107)\n",
      "('hillaryclinton', 0.6027639508247375)\n",
      "('crook', 0.5993449687957764)\n",
      "('slogans4', 0.5369963645935059)\n",
      "('hillari', 0.5149905681610107)\n",
      "('rehir', 0.5136585831642151)\n",
      "('isha', 0.5114279389381409)\n",
      "('webmd', 0.5085569620132446)\n",
      "('candidaci', 0.5001843571662903)\n",
      "('cli', 0.49770089983940125)\n",
      "('cosbi', 0.4947604238986969)\n",
      "('rapey', 0.4941311478614807)\n",
      "('thnx', 0.4740634560585022)\n",
      "('huma', 0.4717243015766144)\n",
      "('maher', 0.4642608165740967)\n",
      "('benghazi', 0.4538607895374298)\n",
      "('berni', 0.45230910181999207)\n",
      "('kristol', 0.4453328847885132)\n",
      "('podesta', 0.438032329082489)\n",
      "('trump', 0.41647055745124817)\n"
     ]
    }
   ],
   "source": [
    "#just an example of the use of the Word2vec Model\n",
    "stemmer = PorterStemmer()\n",
    "word='clinton'\n",
    "try:\n",
    "    print('\\n'.join([str(w) for w in model.wv.most_similar(stemmer.stem(word), topn=20)]))\n",
    "except KeyError:\n",
    "    print('Not in vocabulary or excluded during pre-processing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build extended list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(topics, model, min_similarity=0.6):\n",
    "    \"\"\"Get the words that are similar in the word2vec space. \n",
    "    min_similarity determines the threshold of similarity\n",
    "    RETURNS: an extendent list for each topics\"\"\"\n",
    "    topic_extended=[] #new list of words\n",
    "    \n",
    "    for topic in topics:\n",
    "        new_words=[]\n",
    "       \n",
    "        for word_raw in topic:\n",
    "            #modifies the wors as in the preprocessing\n",
    "            word=stemmer.stem(word_raw.lower())\n",
    "            new_words.append(word_raw)\n",
    "            \n",
    "            try: #if the word is in the vocab\n",
    "                related=model.wv.most_similar(word, topn=30) \n",
    "                new_words+=[related[idx][0] for idx, _ in enumerate(related) if related[idx][1] >= min_similarity]\n",
    "            \n",
    "            except KeyError: #the word is not in the vocabulary (anymore)\n",
    "                #new_words.remove(word_raw) #then remove this word\n",
    "                continue\n",
    "                \n",
    "        topic_extended.append(new_words)\n",
    "    \n",
    "    return topic_extended\n",
    "\n",
    "\n",
    "#We will create a new col of stemmed words of wiki's events to compare with tweets keywords\n",
    "link_numbers=('http', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-') #\n",
    "stemmer = PorterStemmer()\n",
    "stop_w=[word.replace('\\'','') for word in stopwords.words('english')]+ ['', '&amp', 'amp','rt'] \n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Split the the tweet into a list of (cleaned words)\"\"\"\n",
    "    text_cleaned = ''.join(ch for ch in text if ch not in '#!\"$%&\\()*+,./:;<=>?@[\\\\]^_{|}~\\'').split(' ') \n",
    "    \n",
    "    words= [word.lower().encode('ascii',errors='ignore').decode() for word in text_cleaned \\\n",
    "            if not ( (word.startswith(link_numbers)) | (word.endswith(link_numbers)) )]\n",
    "    \n",
    "    words=[stemmer.stem(word) for word in words if word not in stop_w ]\n",
    "        \n",
    "    if len(words) > 0:\n",
    "        return words\n",
    "    else: #tweets that contains only links or emojiis ...\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have chosen 16 topics (129 words):\n",
      "    Music\n",
      "    Trump\n",
      "    Hillary\n",
      "    Religion\n",
      "    Fear\n",
      "    BlackLivesMatters\n",
      "    Voter\n",
      "    MeToo\n",
      "    Hurricane\n",
      "    Terrorism\n",
      "    Hacking\n",
      "    Obama\n",
      "    Money\n",
      "    Election\n",
      "    Charlottesville\n",
      "    Economy\n"
     ]
    }
   ],
   "source": [
    "#the firs word of the list represents the concept of each list.\n",
    "topics_raw=[['Music', 'lyrics', 'album','musician', 'nowplaying', 'soundcloud','rap', 'rnb','hip-hop', 'eminem','nas', 'horsemen', 'dj', 'wayne', 'snoop'],\\\n",
    "        ['Trump', 'melania', 'Donald', 'trump2016','realdonaldtrump', 'anti-trump', 'maga', 'MakeAmericaGreatAgain'],\\\n",
    "        ['Hillary', 'hilliari',  'HeforShe', 'IamWithHer', 'crookedHillary', 'Clinton','killari','neverhillari', ],\\\n",
    "        ['Religion', 'islam', 'christianism', 'judaism'],\\\n",
    "        ['Fear', 'Korea', 'North', 'NorthKorea', 'Russia', 'putin', 'Geopolotics', 'kim', 'jong', 'kimjongun'],\\\n",
    "        ['BlackLivesMatters','BlackLiveMatter', 'black', 'trayvonmartin', 'mikebrown', 'sandrabland', 'bland', 'kaepernick','policeviolence', 'brutality' , 'fuckthepolice', 'cop', \\\n",
    "         'racial', 'blacktwitter', 'colin', 'kaepernick','racism' ],\\\n",
    "        ['Voter', 'fraud'],\\\n",
    "        ['MeToo', 'Weinstein', 'sexual', 'harassment', 'predator', 'feminist'],\\\n",
    "        ['Hurricane', 'Irma', 'matthew', 'flood', 'thunderstorm' ],\\\n",
    "        ['Terrorism', 'terror' 'terrorist', 'parisattack', 'londonattack', 'shootings', 'baltimore', 'bombings', 'Chattanooga', 'prayforbrussel', 'brussel', 'massacre'],\\\n",
    "        ['Hacking', 'emails', 'DNC'],\\\n",
    "        ['Obama', 'barack', 'michelle'],\\\n",
    "        ['Money', 'scandal', 'WellsFargo', 'Fargo', 'ImranAwan', 'Imran', 'Awan', 'Wasserman'],\\\n",
    "        ['Election', 'campaign', 'GOP','gopdebate', 'DNC', 'Dem', 'demdebate', 'vote', 'Ivoted', 'debate', 'primary', 'convention', 'america'],\\\n",
    "        ['Charlottesville','alt-righ', 'alt-left',  'neonazi', 'neo-nazi','kkk', 'klan', 'supremacist', 'unitetheright'], \\\n",
    "        ['Economy', 'deal', 'Nafta','stock', 'market','bitcoin', 'nasdaq']]\n",
    "\n",
    "print('We have chosen {} topics ({} words):\\n    {}'.format(len(topics_raw), len(sum(topics_raw,[])), '\\n    '.join([topic[0] for topic in topics_raw])))\n",
    "\n",
    "topics_treated=[]\n",
    "for topic in topics_raw:\n",
    "    topics_treated.append([word.lower() for word in topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extending the list:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extended list is 1048 long.\n"
     ]
    }
   ],
   "source": [
    "topic_extended=get_related_words(topics_raw, model, min_similarity=0.6)\n",
    "\n",
    "print('The extended list is {} long.'.format(len(sum(topic_extended,[]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorizing tweets** <br>\n",
    "Each tweet with respect to the topic. If a tweet contains a word contained in the list of a certain topic, then it will have a non null composant in this topic column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_topics(df, topic):\n",
    "    \"\"\"Add a column corresponding to each topic. Fills 1 if the tweet has a word \n",
    "    related to the topic, 0 otherwise \"\"\" \n",
    "    \n",
    "    df_tmp=df[df['Clean_tweet'].isnull()==False]['Clean_tweet']\n",
    "    for topic in topic_extended:\n",
    "        df[topic[0]]= df_tmp.apply(lambda words: 1 if len(set(words) & set(topic))> 0 else 0)\n",
    "        df[topic[0]].fillna(0, inplace=True)\n",
    "\n",
    "tweet_topics(df, topic_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting - tweet activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Jean-BaptistePROST/Documents/EPFL/ADA-Proj-18/Plots/Topic_noEvent.html'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_plot = figure(plot_width=950, plot_height=600, x_axis_type='datetime', toolbar_location=\"above\")\n",
    "\n",
    "colorplot=Category20[len(topic_extended)] #form the bokeh palettes\n",
    "start_date = df.publish_date.min()\n",
    "end_date = df.publish_date.max()\n",
    "\n",
    "\n",
    "for color, topic in enumerate(topic_extended): #iterates over each topic\n",
    "    topic_tmp=topic[0]\n",
    "    df_plot=pd.DataFrame(data=df[df[topic_tmp]==1].publish_date.value_counts().sort_index())\n",
    "    \n",
    "    source = ColumnDataSource(data=df_plot)\n",
    "    topic_plt_tmp=topic_plot.line(x='index', y='publish_date', source=source,\\\n",
    "            line_width=2, alpha=0.8, legend=topic_tmp, color=colorplot[color])\n",
    "    \n",
    "    topic_plt_tmp.visible=False\n",
    "\n",
    "topic_plt_tmp.visible=True\n",
    "\n",
    "topic_plot.legend.location = 'top_left'\n",
    "topic_plot.legend.click_policy='hide'\n",
    "topic_plot.title.text = 'Tweeting activity according topic'\n",
    "\n",
    "hover_tool=tools.HoverTool(\n",
    "    tooltips=[\n",
    "        ('Date', '@index{%b %d, %Y}'),\n",
    "        ('Number of tweets','@publish_date')],\n",
    "\n",
    "    formatters={\n",
    "        'index' : 'datetime', # use 'datetime' formatter for 'date' field\n",
    "        'publish_date' : 'printf',   },   # use 'printf' formatter for 'adj close' field\n",
    "\n",
    "    # display a tooltip whenever the cursor is vertically in line with a glyph\n",
    "    mode='vline',\n",
    "    attachment='above',\n",
    "    show_arrow=True,\n",
    ")\n",
    "\n",
    "topic_plot.tools.append(hover_tool)\n",
    "\n",
    "output_file(plot_files+'Topic_noEvent.html')\n",
    "save(topic_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The plot can be seen [here](Plots/Topic_noEvent.html)**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "WORDS_TO_ADD=['donald', 'trump', 'dem', 'gop', 'hillary', 'clinton', 'trayvon', 'tamir', \\\n",
    "              'islam', 'fuck', 'nfl', 'kaepernick', 'dnc', 'charlottesville', 'korea', 'sacramento',\\\n",
    "              'blm', 'alt', 'kkk', 'berkeley', 'wasserman-schultz', 'pence']\n",
    "\n",
    "for word in WORDS_TO_ADD:\n",
    "    word_dictionary.append(word)\n",
    "\n",
    "for alphabet in \"bcdefghjklmnopqrstuvwxyz\":\n",
    "    word_dictionary.remove(alphabet)\n",
    "\n",
    "    \n",
    "def retag(tag):\n",
    "    '''\n",
    "    Receives tags as treebank (VBD) and changes it in wordnet (v) if exsits\n",
    "    '''\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "    \n",
    "def best_hash_split(hashtag, wordfrequency):\n",
    "    '''\n",
    "    Receives a hashtag and the frequency of the most common 5000 english words. If the word or its lemma is\n",
    "    not in dictionnary, then it splits it. Once split, it assess the most probable split based on word freq. \n",
    "    Returns the hashtag if the word (or lemma) is in dictionnary. Returns the best split if it found one.\n",
    "    If it could find satisfactory split, it returns nothing\n",
    "    '''\n",
    "    #getting all possible splits\n",
    "    if (hashtag not in word_dictionary) | (stemmer.stem(hashtag) not in word_dictionary):\n",
    "        all_splits = split_hashtag_to_words_all_possibilities(hashtag)\n",
    "        max_prob = 0\n",
    "        i = 0\n",
    "        best_split = 0\n",
    "        for possible_splits in all_splits:\n",
    "            #tag the splits\n",
    "            word_pos = nltk.pos_tag(possible_splits)\n",
    "            probabilities = []\n",
    "            for (word, pos) in word_pos:\n",
    "                #if the word is already in wordfrequency, get its frequency\n",
    "                if (wordfrequency.Word == word).any():\n",
    "                    probabilities.append(wordfrequency[wordfrequency.Word.str.lower() == word].max()[2])\n",
    "                else:\n",
    "\n",
    "                    freq = (wordfrequency[wordfrequency.Word.str.lower() == lemmatizer.lemmatize\\\n",
    "                                                                            (word, pos=retag(pos))].max()[2])\n",
    "                    #otherwise, if lemma is in wordfrequ get its frequency\n",
    "                    if freq > 0:\n",
    "                        probabilities.append(freq)\n",
    "                    #if not, keep lowest freq we find.\n",
    "                    else : \n",
    "                        probabilities.append(wordfrequency.Frequency.min())\n",
    "            split_prob = np.prod(probabilities)\n",
    "            if split_prob > max_prob:\n",
    "                max_prob = split_prob\n",
    "                best_split = i\n",
    "            i = i + 1\n",
    "        if len(all_splits) != 0:\n",
    "            return all_splits[best_split]  \n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return hashtag\n",
    "               \n",
    "    \n",
    "def split_hashtag_to_words_all_possibilities(hashtag):\n",
    "    '''\n",
    "    Receives a hashtag and returns all the possible splits.\n",
    "    '''\n",
    "    hashtag = hashtag.lower()\n",
    "    all_possibilities = [] \n",
    "    split_posibility = [hashtag[:i] in word_dictionary for i in reversed(range(len(hashtag)+1))]\n",
    "    possible_split_positions = [i for i, x in enumerate(split_posibility) if x == True]\n",
    "    for split_pos in possible_split_positions:\n",
    "        split_words = []\n",
    "        word_1, word_2 = hashtag[:len(hashtag)-split_pos], hashtag[len(hashtag)-split_pos:]\n",
    "        if word_2 in word_dictionary:\n",
    "            split_words.append(word_1)\n",
    "            split_words.append(word_2)\n",
    "            all_possibilities.append(split_words)\n",
    "\n",
    "            another_round = split_hashtag_to_words_all_possibilities(word_2)\n",
    "\n",
    "            if len(another_round) > 0:\n",
    "                all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n",
    "        else:\n",
    "            another_round = split_hashtag_to_words_all_possibilities(word_2)\n",
    "            \n",
    "            if len(another_round) > 0:\n",
    "                all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n",
    "                      \n",
    "    return all_possibilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Wikipedia Portal which lists every event that have happened for each day. The html template is always the same enabling us to efficiently scrap it.\n",
    "\n",
    "\n",
    "\n",
    "We want to detect the event related to the spike of tweets:\n",
    "* For each categry, a threshold is define to localize the date spikes. \n",
    "* Wikipedia Portal is scrapped at the url of the date. The text follows the same pre-process as the tweets words.\n",
    "* A matching of the subsection of the web page and the word of the list is done.\n",
    "* We retrieve the description of the event if it its related to the topic."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#We will create a new col of stemmed words of wiki's events to compare with tweets keywords\n",
    "link_numbers=('http', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-') #\n",
    "stemmer = PorterStemmer()\n",
    "stop_w=[word.replace('\\'','') for word in stopwords.words('english')]+ ['', '&amp', 'amp','rt'] \n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Split the the tweet into a list of (cleaned words)\"\"\"\n",
    "    text_cleaned = ''.join(ch for ch in text if ch not in '#!\"$%&\\()*+,./:;<=>?@[\\\\]^_{|}~\\'').split(' ') \n",
    "    \n",
    "    words= [word.lower().encode('ascii',errors='ignore').decode() for word in text_cleaned \\\n",
    "            if not ( (word.startswith(link_numbers)) | (word.endswith(link_numbers)) )]\n",
    "    \n",
    "    words=[stemmer.stem(word) for word in words if word not in stop_w ]\n",
    "        \n",
    "    if len(words) > 0:\n",
    "        return words\n",
    "    else: #tweets that contains only links or emojiis ...\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting web page\n",
    "WIKI_PORTAL = \"https://en.wikipedia.org/wiki/Portal:Current_events/\"\n",
    "\n",
    "def event_scrapper(date):\n",
    "    \"\"\"Retrieve the information of the events that happened around a date window\"\"\"\n",
    "    \n",
    "    #take two days before and one day after the date\n",
    "    dates = [date + dt.timedelta(day,0) for day in [-1,0]]\n",
    "    #daily events are stored in df\n",
    "    event_df = pd.DataFrame(columns=['Date', 'Description', 'Category', 'Link'])\n",
    "    \n",
    "    for date in dates:\n",
    "        #strftime gives 0-padded days...\n",
    "        year = date.strftime(\"%Y\")\n",
    "        month = date.strftime(\"%B\")\n",
    "        day = re.sub(\"^[0]\", \"\", date.strftime(\"%d\"))  #removing 0 at beggining of day\n",
    "\n",
    "        #right format for wiki portal and requesting html\n",
    "        url_date = year + \"_\" + month + \"_\" + day\n",
    "        r = requests.get(WIKI_PORTAL + url_date)\n",
    "        \n",
    "        #print('Response status code: {0}\\n'.format(r.status_code))\n",
    "        \n",
    "        page_body = r.text\n",
    "        soup = BeautifulSoup(page_body, 'html.parser')\n",
    "\n",
    "        #For every category of events (dt), we look for every events from bullet list (li)\n",
    "        if soup.findAll(\"dt\"):\n",
    "            \n",
    "            for category in soup.findAll(\"dt\"):\n",
    "\n",
    "                li = category.findNext(\"li\")\n",
    "                while li:\n",
    "                    #replacing \\n, and removing sources: text. e.g. (CNN). \n",
    "                    #we decided to split text using regex and keeping only text before first source\n",
    "                    full_text = re.split(\".\\s\\(\", li.getText().replace('\\n', '. '))\n",
    "                    no_source = full_text[0]\n",
    "                    new_event = pd.DataFrame({'Date': [date], \\\n",
    "                                              'Description': [no_source],\\\n",
    "                                              'Category': [category.getText()], \\\n",
    "                                              'Link': [li.a]})\n",
    "\n",
    "\n",
    "                    event_df = event_df.append(new_event, ignore_index=True, sort=False)\n",
    "                    li = li.findNextSibling(\"li\")\n",
    "            \n",
    "            #event_df['Stemmed_Content']=event_df.apply(lambda row: tokenize(row['Description']), axis=1)\n",
    "            \n",
    "        \n",
    "        else: #other template of wiki portal \n",
    "            for category in soup.findAll(\"p\"):\n",
    "                category_title=category.findNext().getText()\n",
    "                li = category.findNext(\"li\")\n",
    "                while li:\n",
    "  \n",
    "                    full_text = re.split(\".\\s\\(\", li.getText().replace('\\n', '. '))\n",
    "                    no_source = full_text[0]\n",
    "                    new_event = pd.DataFrame({'Date': [date], \\\n",
    "                                              'Description': [no_source],\\\n",
    "                                              'Category': [category_title], \\\n",
    "                                              'Link': [li.a]})\n",
    "\n",
    "\n",
    "                    event_df = event_df.append(new_event, ignore_index=True, sort=False)\n",
    "                    li = li.findNextSibling(\"li\")\n",
    "\n",
    "        \n",
    "    #stemming the content of the events description\n",
    "    event_df['Stemmed_Content']=event_df.apply(lambda row: tokenize(row['Description']), axis=1)\n",
    "\n",
    "    return event_df\n",
    "\n",
    "\n",
    "def event_detector(df, topic_extended):\n",
    "    \"\"\"Detect the event by web scrapping. Returns the dataframe of events' description\n",
    "    per day, the topic and the recall of the process (number of event that match over\n",
    "    the number of tweet spikes)\"\"\"\n",
    "    print('Srapping ...')\n",
    "    \n",
    "    events_df=pd.DataFrame(columns=['Date', 'Topic', 'Event', 'Stemmed'])\n",
    "    nb_dates=0\n",
    "    nb_detected=0\n",
    "    \n",
    "    for topic in topic_extended:\n",
    "        topic_tmp=topic[0]\n",
    "        print('  _ '+topic_tmp)\n",
    "        \n",
    "        #create dataframe\n",
    "        df_topic=pd.DataFrame(data=df[df[topic_tmp]==1].publish_date.value_counts().sort_index().reset_index())\n",
    "        df_topic.rename(columns={'index':'Date', 'publish_date': 'Count'}, inplace=True)\n",
    "\n",
    "        #get date  where tweet activity is above the thresold\n",
    "        threshold=df_topic.Count.mean() + 2.25*df_topic.Count.std()\n",
    "        dates_thr=df_topic[df_topic.Count >= threshold].index\n",
    "        #less than threshold?\n",
    "        dates=[df_topic.loc[date,'Date'] for date in dates_thr if df_topic.loc[date-1,'Count'] < threshold]\n",
    "        \n",
    "        for date in dates:\n",
    "            nb_dates+=1 #counter for recall score\n",
    "            event=event_scrapper(date)\n",
    "            \n",
    "            \n",
    "            try: \n",
    "                #matching the description and the words of the topic\n",
    "                matching_score=event.Stemmed_Content.apply(lambda words: len(set(words) & set(topic)))\n",
    "                max_match=matching_score.max() \n",
    "                #print(max_match)\n",
    "                \n",
    "                if (max_match > 0) : #avoid empty results\n",
    "                    nb_detected+=1#counter for recall score\n",
    "                   \n",
    "                    #if two descriptions have the same matching score\n",
    "                    for idx_max_match in matching_score[matching_score==max_match].index:\n",
    "                    \n",
    "                        tmp=pd.DataFrame({'Date': [event.loc[idx_max_match, 'Date']], \\\n",
    "                                          'Topic': [topic_tmp], \\\n",
    "                                          'Event': [event.loc[idx_max_match,'Description']]})\n",
    "                        events_df=events_df.append(tmp, ignore_index=True, sort=False)\n",
    "                        \n",
    "            \n",
    "            except AttributeError: #track of potential error\n",
    "                print(date)\n",
    "                continue\n",
    "            \n",
    "            except TypeError:\n",
    "                print('No content has been found during scrap')\n",
    "    \n",
    "    \n",
    "    events_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return events_df, nb_detected/nb_dates, \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Srapping ...\n",
      "  _ Music\n",
      "  _ Trump\n",
      "  _ Hillary\n",
      "  _ Religion\n",
      "  _ Fear\n",
      "  _ BlackLivesMatters\n",
      "  _ Voter\n",
      "  _ MeToo\n",
      "  _ Hurricane\n",
      "  _ Terrorism\n",
      "  _ Hacking\n",
      "  _ Obama\n",
      "  _ Money\n",
      "  _ Election\n",
      "  _ Charlottesville\n",
      "  _ Economy\n"
     ]
    }
   ],
   "source": [
    "events_df, recall= event_detector(df, topic_extended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event detector matched 61.03% (134) of the tweet peak\n"
     ]
    }
   ],
   "source": [
    "print('The event detector matched {:0.2f}% ({}) of the tweet peak'.format(100*recall, events_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jean-BaptistePROST/miniconda3/envs/ada/lib/python3.6/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "plot_threshold=True\n",
    "\n",
    "start_date = df.publish_date.min()\n",
    "end_date = df.publish_date.max()\n",
    "\n",
    "topic_plot = figure(plot_width=950, plot_height=600, x_axis_type='datetime',\\\n",
    "                    y_range=[-10,4800], toolbar_location=None)\n",
    "\n",
    "colorplot=Category20[len(topic_extended)] #form the bokeh palettes\n",
    "\n",
    "ymax=[]\n",
    "legend_tmp=[]\n",
    "\n",
    "\n",
    "for color, topic in enumerate(topic_extended): #iterates over each topic\n",
    "    topic_tmp=topic[0]\n",
    "    \n",
    "    #tweets\n",
    "    df_plot=pd.DataFrame(data=df[df[topic_tmp]==1].publish_date.value_counts().sort_index())\n",
    "    ymax.append(df_plot.publish_date.max())\n",
    "    \n",
    "    \n",
    "    if plot_threshold:\n",
    "        threshold=df_plot.publish_date.mean() + 2* df_plot.publish_date.std()\n",
    "        threshold_plt_tmp=topic_plot.line(x=[start_date,end_date],\\\n",
    "                                      y=[threshold,threshold],\\\n",
    "                                      color=colorplot[color], alpha=0.5, )\n",
    "        threshold_plt_tmp.visible=False\n",
    "    \n",
    "    source = ColumnDataSource(data=df_plot)\n",
    "    topic_plt_tmp=topic_plot.line(x='index', y='publish_date', source=source,\\\n",
    "            line_width=2, alpha=0.9, color=colorplot[color])\n",
    "    \n",
    "    hover_tool=tools.HoverTool(\n",
    "        tooltips=[\n",
    "            ('Date', '@index{%b %d, %Y}'),\n",
    "            ('Number of tweets','@publish_date')],\n",
    "        formatters={\n",
    "            'index' : 'datetime', \n",
    "            'publish_date' : 'printf',},   \n",
    "        mode='vline',\n",
    "        attachment='above',\n",
    "        renderers=[topic_plt_tmp])\n",
    "\n",
    "    topic_plot.tools.append(hover_tool)\n",
    "    topic_plt_tmp.visible=False\n",
    "\n",
    "    #events\n",
    "    event_plot=events_df[events_df['Topic']==topic_tmp]\n",
    "    event_plot['Y']=4500\n",
    "    source_event=ColumnDataSource(data=event_plot)\n",
    "    event_plot_tmp=topic_plot.scatter(x='Date', y='Y', source=source_event,\\\n",
    "                                       fill_color=colorplot[color], color=None, size=10, alpha=0.5)\n",
    "    \n",
    "    hover_event=tools.HoverTool(\n",
    "        tooltips=[('Event', '@Event'),\\\n",
    "                  ('Date', '@Date{%b %d, %Y}')],\n",
    "        formatters={'Event' : 'printf', 'Date' : 'datetime' },  \n",
    "        mode=\"vline\",\n",
    "        attachment='right',\n",
    "        show_arrow=False,\n",
    "        renderers=[event_plot_tmp])\n",
    "\n",
    "    topic_plot.tools.append(hover_event)\n",
    "    event_plot_tmp.visible=False\n",
    "    \n",
    "    legend_tmp.append((topic_tmp,[threshold_plt_tmp, topic_plt_tmp,  event_plot_tmp]))\n",
    "    \n",
    "    #displays only the first line at the begining\n",
    "    if color==0:\n",
    "        threshold_plt_tmp.visible=True\n",
    "        event_plot_tmp.visible=True\n",
    "        topic_plt_tmp.visible=True\n",
    "\n",
    "\n",
    "\n",
    "topic_plot.legend.location = 'top_left'\n",
    "topic_plot.legend.orientation = \"vertical\"\n",
    "topic_plot.legend.click_policy='hide'\n",
    "topic_plot.legend.label_text_font_size='8pt'\n",
    "\n",
    "topic_plot.title.text = 'Tweeting activity according topic'\n",
    "topic_plot.yaxis.bounds=(0,max(ymax))\n",
    "\n",
    "#output_notebook()\n",
    "\n",
    "legend = Legend(items=legend_tmp,\\\n",
    "               click_policy='hide')\n",
    "topic_plot.add_layout(legend, 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(topic_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Jean-BaptistePROST/Documents/EPFL/ADA-Proj-18/Plots/Topic_Events.html'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file(plot_files+'Topic_Events.html')\n",
    "save(topic_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot can be seen [here](Plots/Topic_Events.html)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, words\n",
    "nltk.download('words')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "WORDS_TO_ADD=['donald', 'trump', 'dem', 'gop', 'hillary', 'clinton', 'trayvon', 'tamir', 'rice' \\\n",
    "              'islam', 'fuck', 'nfl', 'kaepernick', 'dnc', 'charlottesville', 'korea', 'sacramento',\\\n",
    "              'blm', 'alt', 'kkk', 'berkeley', 'music', 'michael', 'brown', 'mike']\n",
    "word_dictionary = list(set(words.words()))\n",
    "\n",
    "for word in WORDS_TO_ADD:\n",
    "    word_dictionary.append(word)\n",
    "\n",
    "for alphabet in \"bcdefghjklmnopqrstuvwxyz\":\n",
    "    word_dictionary.remove(alphabet)\n",
    "\n",
    "    \n",
    "def retag(tag):\n",
    "    '''\n",
    "    Receives tags as treebank (VBD) and changes it in wordnet (v) if exsits\n",
    "    '''\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    \n",
    "    \n",
    "def best_hash_split(hashtag, wordfrequency):\n",
    "    '''\n",
    "    Receives a hashtag and the frequency of the most common 5000 english words. If the word or its lemma is\n",
    "    not in dictionnary, then it splits it. Once split, it assess the most probable split based on word freq. \n",
    "    Returns the hashtag if the word (or lemma) is in dictionnary. Returns the best split if it found one.\n",
    "    If it could find satisfactory split, it returns nothing\n",
    "    '''\n",
    "    #getting all possible splits\n",
    "    if (hashtag not in word_dictionary) | (stemmer.stem(hashtag) not in word_dictionary):\n",
    "        all_splits = split_hashtag_to_words_all_possibilities(hashtag)\n",
    "        max_prob = 0\n",
    "        i = 0\n",
    "        best_split = 0\n",
    "        for possible_splits in all_splits:\n",
    "            #tag the splits\n",
    "            word_pos = nltk.pos_tag(possible_splits)\n",
    "            probabilities = []\n",
    "            for (word, pos) in word_pos:\n",
    "                #if the word is already in wordfrequency, get its frequency\n",
    "                if (wordfrequency.Word == word).any():\n",
    "                    probabilities.append(wordfrequency[wordfrequency.Word.str.lower() == word].max()[2])\n",
    "                else:\n",
    "\n",
    "                    freq = (wordfrequency[wordfrequency.Word.str.lower() == lemmatizer.lemmatize\\\n",
    "                                                                            (word, pos=retag(pos))].max()[2])\n",
    "                    #otherwise, if lemma is in wordfrequ get its frequency\n",
    "                    if freq > 0:\n",
    "                        probabilities.append(freq)\n",
    "                    #if not, keep lowest freq we find.\n",
    "                    else : \n",
    "                        probabilities.append(wordfrequency.Frequency.min())\n",
    "            split_prob = np.prod(probabilities)\n",
    "            if split_prob > max_prob:\n",
    "                max_prob = split_prob\n",
    "                best_split = i\n",
    "            i = i + 1\n",
    "            print(max_prob)\n",
    "        if (len(all_splits) != 0) & (max_prob > np.exp(-25)):            \n",
    "            return all_splits[best_split]  \n",
    "        else:\n",
    "            return [hashtag]\n",
    "    else:\n",
    "        return [hashtag]\n",
    "               \n",
    "    \n",
    "def split_hashtag_to_words_all_possibilities(hashtag):\n",
    "    '''\n",
    "    Receives a hashtag and returns all the possible splits.\n",
    "    '''\n",
    "    hashtag = hashtag.lower()\n",
    "    all_possibilities = [] \n",
    "    split_posibility = [hashtag[:i] in word_dictionary for i in reversed(range(len(hashtag)+1))]\n",
    "    possible_split_positions = [i for i, x in enumerate(split_posibility) if x == True]\n",
    "    for split_pos in possible_split_positions:\n",
    "        split_words = []\n",
    "        word_1, word_2 = hashtag[:len(hashtag)-split_pos], hashtag[len(hashtag)-split_pos:]\n",
    "        if word_2 in word_dictionary:\n",
    "            split_words.append(word_1)\n",
    "            split_words.append(word_2)\n",
    "            all_possibilities.append(split_words)\n",
    "\n",
    "            another_round = split_hashtag_to_words_all_possibilities(word_2)\n",
    "\n",
    "            if len(another_round) > 0:\n",
    "                all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n",
    "        else:\n",
    "            another_round = split_hashtag_to_words_all_possibilities(word_2)\n",
    "            \n",
    "            if len(another_round) > 0:\n",
    "                all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n",
    "                      \n",
    "    return all_possibilities\n",
    "\n",
    "new_topic_extended = [hashtag_splitter(hashlist) for hashlist in topic_extended]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets' distribution statisitcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Categorical'] = np.sum(df[df.columns[5:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['publish_date', 'content', 'account_category', 'Clean_tweet',\n",
       "       'hashtags', 'Music', 'Trump', 'Hillary', 'Religion', 'Fear',\n",
       "       'BlackLivesMatters', 'Voter', 'MeToo', 'Hurricane', 'Terrorism',\n",
       "       'Hacking', 'Obama', 'Money', 'Election', 'Charlottesville', 'Economy',\n",
       "       'Categorical'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_distribution(df, topic):\n",
    "    '''\n",
    "    Returns the percentage of tweets by topic\n",
    "    '''\n",
    "    results = pd.DataFrame()\n",
    "    tot = len(df)\n",
    "    tot_r = len(df[df.account_category == 'RightTroll'])\n",
    "    tot_l = len(df[df.account_category == 'LeftTroll'])\n",
    "    \n",
    "    for i in range(len(topic)):\n",
    "        results.loc[df.columns[i+5], 'Percent'] = np.round(100*(df[df.columns[i+5]].sum()/tot),2)\n",
    "        results.loc[df.columns[i+5], 'Percent_Right'] = np.round(100*(df[df.account_category == 'RightTroll'][df.columns[i+5]].sum()/tot_r),2)\n",
    "        results.loc[df.columns[i+5], 'Percent_Left'] = np.round(100*(df[df.account_category == 'LeftTroll'][df.columns[i+5]].sum()/tot_l),2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def uncategorized_vocab(tweets, min_occurence=30):\n",
    "    \"\"\"Gathers all the words that are more frequent than min_occurence\"\"\"\n",
    "    voc_raw=[]\n",
    "    for words in tweets:\n",
    "        if words: #if not None\n",
    "            voc_raw+=words # add to voca\n",
    "    count_=Counter(voc_raw) #count frequency\n",
    "    uncat_voc = pd.DataFrame.from_dict(count_, orient='index').reset_index()\n",
    "    uncat_voc = uncat_voc.rename(columns={'index':'word', 0:'count'})\n",
    "    uncat_relevant=pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(uncat_voc)):\n",
    "        if (uncat_voc.iloc[i]['word'] not in stop_w) & (len(uncat_voc.iloc[i]['word']) > 4):\n",
    "            uncat_relevant.loc[i, 'Word'] = uncat_voc.iloc[i]['word']\n",
    "            uncat_relevant.loc[i, 'Count'] = uncat_voc.iloc[i]['count']\n",
    "\n",
    "    \n",
    "    return uncat_relevant\n",
    "\n",
    "def get_uncat_voc(tweets, min_count=30):\n",
    "    \"\"\"Recovers the vocabulary from pickles\"\"\"\n",
    "    filename=pickle_files+'uncat_vocabulary_'+str(min_count)+'.pkl'\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'rb') as uncat_voc:\n",
    "            uncat_vocabulary=pickle.load(uncat_voc) \n",
    "        print(\"Vocab loaded <---\")\n",
    "            \n",
    "    except: \n",
    "        uncat_vocabulary= uncategorized_vocab(tweets)\n",
    "        \n",
    "        with open(filename, 'wb') as uncat_voc:\n",
    "            pickle.dump(uncat_vocabulary, uncat_voc, pickle.HIGHEST_PROTOCOL) #saving the voc\n",
    "            \n",
    "    return uncat_vocabulary\n",
    "\n",
    "def save_topic_df(df, filename=pickle_files +'topic_df.plk'):\n",
    "   \n",
    "    with open(filename, 'wb') as df_top:\n",
    "        pickle.dump(df, df_top, pickle.HIGHEST_PROTOCOL)\n",
    "    print('Dateframe saved -->')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorized tweets represent: 45.59% of all the tweets. However, 'RighTroll' tweets are categorized in 49.25%, against 39.36 for 'LeftTroll'\n",
      "\n",
      "45.59% of all the tweets appear in more than one category.\n",
      "\n",
      "0.50% of all the tweets are 'empty'.\n",
      "\n",
      "Hashtags are present in: 31.65% of all the tweets, 'RightTrolls' rely more on hashtags, including them in : 34.45% of their tweets, compared with 26.88% for 'LeftTrolls'\n"
     ]
    }
   ],
   "source": [
    "topic_proportion = get_category_distribution(df, topic_extended)\n",
    "cat_tweet = (100*len(df[df.Categorical > 0])/len(df))\n",
    "r_cat_tweet = (100*len(df[(df.Categorical > 0) & (df.account_category == 'RightTroll')]) / len(df[df.account_category == 'RightTroll']))\n",
    "l_cat_tweet = 100*len(df[(df.Categorical > 0) & (df.account_category == 'LeftTroll')]) / len(df[df.account_category == 'LeftTroll'])\n",
    "print(\"Categorized tweets represent: {:0.2f}% of all the tweets. However, 'RighTroll' tweets are categorized in {:0.2f}%, against {:0.2f} for 'LeftTroll'\".format(cat_tweet, r_cat_tweet, l_cat_tweet))\n",
    "\n",
    "more_cat = (100*len(df[df.Categorical > 1])/len(df))\n",
    "print('\\n{:0.2f}% of all the tweets appear in more than one category.'.format(more_cat))\n",
    "\n",
    "empty_tweets = (100*len(df[df.Clean_tweet.isnull()])/len(df))\n",
    "print(\"\\n{:0.2f}% of all the tweets are 'empty'.\".format(empty_tweets))\n",
    "\n",
    "df['hash_cat'] = df.hashtags.apply(lambda x: 1 if len(x) > 0 else 0)\n",
    "hash_per = (100*df.hash_cat.sum()/len(df))\n",
    "hash_per_r = (100*df[df.account_category == 'RightTroll'].hash_cat.sum()/len(df[df.account_category=='RightTroll']))\n",
    "hash_per_l = (100*df[df.account_category == 'LeftTroll'].hash_cat.sum()/len(df[df.account_category=='LeftTroll']))\n",
    "print(\"\\nHashtags are present in: {:0.2f}% of all the tweets, 'RightTrolls' rely more on hashtags, including them in : {:0.2f}% of their tweets, compared with {:0.2f}% for 'LeftTrolls'\"\n",
    "      .format(hash_per, hash_per_r, hash_per_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_proportion.to_csv('TopicTroll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Percent</th>\n",
       "      <th>Percent_Right</th>\n",
       "      <th>Percent_Left</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>2.95</td>\n",
       "      <td>0.59</td>\n",
       "      <td>6.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trump</th>\n",
       "      <td>15.63</td>\n",
       "      <td>20.61</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hillary</th>\n",
       "      <td>3.96</td>\n",
       "      <td>5.76</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Religion</th>\n",
       "      <td>2.63</td>\n",
       "      <td>3.62</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fear</th>\n",
       "      <td>2.54</td>\n",
       "      <td>2.98</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BlackLivesMatters</th>\n",
       "      <td>8.45</td>\n",
       "      <td>4.54</td>\n",
       "      <td>15.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voter</th>\n",
       "      <td>2.11</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MeToo</th>\n",
       "      <td>2.56</td>\n",
       "      <td>2.33</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hurricane</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terrorism</th>\n",
       "      <td>3.12</td>\n",
       "      <td>3.59</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hacking</th>\n",
       "      <td>0.86</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obama</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Money</th>\n",
       "      <td>0.83</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Election</th>\n",
       "      <td>10.62</td>\n",
       "      <td>13.54</td>\n",
       "      <td>5.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlottesville</th>\n",
       "      <td>2.06</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economy</th>\n",
       "      <td>1.62</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Percent  Percent_Right  Percent_Left\n",
       "Music                 2.95           0.59          6.97\n",
       "Trump                15.63          20.61          7.17\n",
       "Hillary               3.96           5.76          0.88\n",
       "Religion              2.63           3.62          0.93\n",
       "Fear                  2.54           2.98          1.79\n",
       "BlackLivesMatters     8.45           4.54         15.11\n",
       "Voter                 2.11           2.56          1.35\n",
       "MeToo                 2.56           2.33          2.94\n",
       "Hurricane             0.65           0.78          0.43\n",
       "Terrorism             3.12           3.59          2.33\n",
       "Hacking               0.86           1.03          0.56\n",
       "Obama                 0.45           0.52          0.31\n",
       "Money                 0.83           1.06          0.44\n",
       "Election             10.62          13.54          5.66\n",
       "Charlottesville       2.06           2.55          1.23\n",
       "Economy               1.62           1.80          1.33"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax=topic_proportion[['Percent_Right','Percent_Left']].plot.bar(stacked=True, \\\n",
    "                                                               figsize=(18,8), color=['tomato', 'dodgerblue'])\n",
    "ax.set(ylabel='Spearman coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncat_vocabulary=get_uncat_voc(df[df.Categorical == 0].Clean_tweet, min_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncat_vocabulary.sort_values(by='Count', ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_topic_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-Topic-# : ['Music', 'song', 'hip-hop', 'lyrics', 'jcolenc', 'quavostuntin', 'migo', 'tidalhifi', 'raekwon', 'snoopdogg', 'jcole', 'remast', 'trvisxx', 'solang', 'rappersiq', 'hip-hop', 'rapsodi', 'aguilera', 'bigboi', 'pushat', 'funkflex', 'desiign', 'dmx', 'collab', 'kweli', 'chrisbrown', 'treysongz', 'youngthug', 'pusha', 'trippi', 'nickiminaj', 'remix', 'fwm', 'frenchmontana', 'album', 'ep', 'platinum', 'chancetherapp', 'eyez', 'mixtap', 'donaldglov', 'musician', 'unsign', 'tymon', 'hip-hop', 'storytel', 'songwrit', 'dondatta', 'dancehal', 'edarababyg', 'rampb', 'soni', 'compos', 'tidalhifi', 'jazz', 'self-taught', 'rb', 'vinyl', 'mogul', 'grime', 'disc', 'showcas', 'hoffa', 'poet', 'regga', 'artwork', 'underr', 'spotifi', 'aampr', 'futurist', 'photographi', 'opera', 'nowplaying', 'dagr8fm', 'djshinnergmailcom', 'bryson', 'tiller', 'dej', 'beatz', 'intro', 'mikewillmadeit', 'jeremih', 'rockiefresh', 'powertalkwog', 'mikeydollaz', 'prod', 'akon', 'trina', 'jeezi', 'bigsean', 'otgenasi', 'boosi', 'loaf', 'killa', 'dollaz', 'pushat', 'djshinner', 'swizz', 'regina', 'chainz', 'torylanez', 'makin', 'ludacri', 'soundcloud', 'datpiff', 'audiomack', 'buynow', 'click', 'musicpromo', 'top8', 'limitedtim', 'recordd', 'promo', 'bandcamp', 'getabuzz', 'widget', 'repost', 'livemixtap', 'rap', 'hiphop', 'onli', 'rnb', 'dancehal', 'topstarhiphopra', 'ricorecklezz', 'tidalhifi', 'groov', 'futureofnewyork', 'nervedjsmixtap', 'djkingassassin', 'kingbon', 'djquestgh', 'beatz', 'treysongz', 'tidal', 'hhchiphop', 'youngthug', 'breeziefbabi', 'djgumba', 'hiphopgod', 'topstarhiphop', 'powertalkwog', 'top20', 'dcloud9devi8tor', 'salsa', 'outro', 'tshirt', 'djhivolum', 'n1', 'rappersiq', 'regga', 'quavostuntin', 'hip-hop', 'tidalhifi', 'dancehal', 'rnb', 'musician', 'rampb', 'regga', 'soni', 'showcas', 'tidal', 'tymon', 'duo', 'visual', 'rb', 'djhivolum', 'ballerina', 'disc', 'rotat', 'storytel', 'unsign', 'okayplay', 'artwork', 'gorillaz', 'jazz', 'livemixtap', 'x-men', 'hiphopgod', 'djquestgh', 'emce', 'opera', 'debut', 'eminem', 'nas', 'sade', 'djsyonkream', 'tidalhifi', 'raekwon', 'youngthug', 'rapsodi', 'delux', 'quavostuntin', 'djkhale', 'jcolenc', 'asvpxrocki', 'mikewillmadeit', 'daveeast', 'frenchmontana', 'realremyma', 'donaldglov', 'trvisxx', 'dollaz', 'pandoraradio', 'tydollasign', 'lordlumin', 'bigboi', 'yg', 'outkast', 'pandoramus', 'boogi', 'beatz', 'prod', 'wizkhalifa', 'mikey', 'horsemen', 'mashup', 'hiphopgoldenera', 'hiphophead', 'hiphopmus', 'grafitti', 'twitart', 'oldschool', 'shadetreekde', 'k-dee', 'jeffstonesmus', 'dualiti', 'baphomet', 'basquiat', 'fineart', 'horseman', 'smokedza', 'plata', 'ralofamgoon', 'supa', 'tug', 'mavi', 'flora', 'whitehead', 'dishwash', 'deja', 'maino', 'psychedel', 'wallstreet', 'cuzzi', 'jacqu', 'dj', 'mixshow', 'djkennistarr', 'djquestgh', 'skroogmkduk', 'djtonyhard', 'og', 'bvhk', 'officialcoredj', 'jermainedupri', 'topstarhiphopra', 'powertalkwog', 'topstarhiphop', 'youngthug', 'mrchuckd', 'top20', 'collab', 'hiphopgod', 'bizdatroof', 'deejayalexm', 'vybe', 'js', 'djgumba', 'rnb', 'ricorecklezz', 'k100radio', 'yfnlucci', 'rotat', 'riddim', 'realsway', 'nervedjsmixtap', 'wayne', 'durk', 'yachti', 'bibbi', 'birdman', 'boosi', 'uzi', 'duval', 'snoop', 'dogg', 'hotnewhiphop', 'wiz', 'khalifa', 'boosi', 'tyga', 'badazz', 'lilyachti', 'teelonzo', 'lhh', 'desiign', 'youngthug', 'aap', 'dmx', 'fucc', 'quik', 'realremyma', 'offsetyrn', 'keke', 'frenchmontana', 'yg', 'checkitout', 'juelz', 'iamakademik', 'redbon', 'jeremih', 'trvisxx', 'vu', 'ludacri', 'sizzl']\n",
      "-----\n",
      "#-Topic-# : ['Trump', 'realdonaldtrump', 'melania', 'ivanka', 'barron', 'ivana', 'Donald', 'presid', 'president-elect', 'trump2016', 'pence16', 'pence2016', 'maga', 'trumptrain', 'trump2020', 'elections2016', 'women4', 'realdonaldtrump', 'potu', 'trump', 'anti-trump', 'maga', 'trump2020', 'teamtrump', 'trumptrain', 'trump2016', 'MakeAmericaGreatAgain', 'trumptrain', 'basketofdeplor', 'americafirst', 'votetrump', 'tpot', 'y2', '#trumpwhitehous', 'trump4', 'coreyforva', 'masa', 'trump2020', 'hillaryin', 'imwithh', 'hillaryforprison2016', 'rhillari', 'markcuban', 'teamtrump', '#maga', 'k16', 'littlebytesnew', 'prayerfor', 'captncnk', 'neve', 'sawmilltat', 'cjcboi', 'meetup', 'orpuw', 'ycot', 'electionday', 'neverhillari']\n",
      "-----\n",
      "#-Topic-# : ['Hillary', 'hrc', 'hillaryclinton', 'killari', 'hilliari', 'hilli', 'skank', 'HeforShe', 'IamWithHer', 'bernie2016', 'snob', 'crookedHillary', 'fluffer', 'hillary4', 'imwithh', 'neverhillari', 'hillaryfor', 'hillary2016', 'hillaryforprison', 'hillaryforprison2016', 'illari', 'killari', 'hillary4prison', 'cleak', 'dn', 'Clinton', 'hrc', 'hillaryclinton', 'killari', 'crookedhillari', 'hil', 'hillari', 'hillaryclinton', 'hillaryforprison2016', 'neverhillari', 'hillaryforprison2016', 'hillaryforprison', 'draintheswamp', 'hillary4prison', 'lockherup', 'shillari', 'imwithh', 'hillary2016', 'makeamericagreatagain', 'crookedhillari', 'imnotwithh', 'riggedelect', 'votetrump', 'electionday', '#maga', 'fluffer', 'trumptrain', 'hillary4', 'rhillari', 'indivisibleteam', 'swingleft', 'basketofdeplor', 'usaneedstrump', 'prison2016', 'neve', 'pence2016', 'elections2016', 'illari', 'clintonemail', 'k16']\n",
      "-----\n",
      "#-Topic-# : ['Religion', 'belief', 'infidel', 'ideolog', 'islam', 'non-muslim', 'secular', 'islam', 'islamist', 'muslim', 'infidel', 'religion', 'jihadist', 'non-muslim', 'christianism', 'homosexu', 'cathol', 'atheist', 'jew', 'persecut', 'judaism', 'fundamentalist', 'atheism', 'orthodox']\n",
      "-----\n",
      "#-Topic-# : ['Fear', 'Korea', 'korean', 'nkorea', 'nk', 'North', 'south', 'n', 'NorthKorea', 'Russia', 'russian', 'putin', 'kremlin', 'trumprussia', 'ukrain', 'collus', 'trump-russia', 'putin', 'russia', 'russian', 'vladimir', 'kremlin', 'us-russia', 'diplomat', 'Geopolotics', 'kim', 'jong-un', 'jong', 'jong-nam', 'kardashian', 'burrel', 'noko', 'kourtney', 'dotcom', 'jong', 'kim', 'jong-un', 'jong-nam', 'jung', 'burrel', 'dotcom', 'noko', 'kourtney', 'kratdashian', 'haley', 'kimjongun']\n",
      "-----\n",
      "#-Topic-# : ['BlackLivesMatters', 'notmypresid', 'trumpprotest', 'policeviol', 'policebrut', 'blackpow', 'blc', 'blackfriday14', 'dropthecharg', 'BlackLiveMatter', 'rastasnoddi', 'blackbruin', 'trueislam', 'mrtyson', 'mydailymanna', 'diopta', 'policebelik', 'peopledontgetit', 'niggatheori', 'civilrightsmov', 'strage', 'newsandstock', 'protestpoetri', 'crakk', 'allcopsarebad', 'acrad', 'shooter2x', 'lifeforcevib', 'yourlifematt', 'wraith', 'nought', 'daaaamn', 'blackcrim', 'historia', 'confl', 'adefend', 'unbeliv', 'djchrisjamaica', 'renenow', 'gaiagaudenzi', 'black', 'blk', 'trayvonmartin', 'neverforget', 'iuic', 'treyvon', 'alllivesmat', 'leahntorr', 'sunwalksonwat', 'con-artist', 'kiki', 'tharealversac', 'ssgt', 'lfebr', 'git', 'time100', 'wolftyla', 'happybirthday', 'whitelivesmatt', 'mvzexplor', 'nevil', 'feministabul', 'travon', 'seul', 'blkvoic', 'blaze1theston', 'blackprid', 'franktmcveeti', 'diallo', 'nublackvis', 'jr7jc', 'pbuh', 'mamba', 'mikebrown', 'sandrabland', 'sophiewoolley', 'diopta', 'markgkirshn', 'wraith', 'blackppl', 'protestpoetri', 'blacklivematt', 'bexolog', 'rula', 'skepta', 'nought', 'crimesmelan', 'jihadist2ndwif', 'realdjpremi', 'peopledontgetit', 'niggatheori', 'tharealversac', 'rastasnoddi', 'justbefreexo', 'acrad', 'strage', 'blackbruin', 'mexicanlivesmatt', 'dchomo', 'texasflyty', 'sett', 'shooter2x', 'macki', 'lifeforcevib', 'dilrubale', 'bland', 'sandra', 'bullock', 'kaepernick', 'cowherd', 'colin', 'nfl', 'policeviolence', 'policeabus', 'blackpeopleproblem', 'blacktw', 'blc', 'policest', 'blackpeopletwitt', 'allcopsarebastard', 'blackandwhit', 'fuckthepolic', 'acab', 'paulon', 'blackpeopl', 'blackpow', 'whitesupremaci', 'blkvoic', 'problack', 'blackfriday14', 'antiracismtv', 'korryngain', 'xpression', 'tharealversac', 'altonsterl', 'blackprid', 'alllivesmat', 'blackpeoplebelik', 'wolftyla', 'anoncopwatch', 'asianlivesmatt', 'policebrut', 'blackmattersu', 'brutality', 'fuckthepolice', 'allcopsarebastard', 'blacktw', 'mexicanlivesmatt', 'asianlivesmatt', 'policeabus', 'blackpeoplebelik', 'diopta', 'antiracismtv', 'newsandstock', 'allcopsarebad', 'vallejo', 'whitelivesmatt', 'chvi', 'tharealversac', 'dchomo', 'blackandwhit', 'bexolog', 'blackpeopleproblem', 'f4f', 'blackprid', 'badcop', 'twocran', 'protestpoetri', 'franceafriqu', 'korryngain', 'democratg', 'rula', 'jaybewar', 'blackgirl', 'democratgermfactori', 'cop', 'polic', 'policeman', 'policebrut', 'unarm', 'racial', 'discrimin', 'implicit', 'slur', 'blacktwitter', 'blackpow', 'blackcultur', 'blacknew', 'blackhistori', 'blackhistorymonth', 'blackpeopleproblem', 'blc', 'colin', 'kaepernick', 'kaepernick', 'cowherd', 'colin', 'nfl', 'racism', 'hatr', 'bigotri', 'injustic', 'sexism', 'oppress', 'homophobia']\n",
      "-----\n",
      "#-Topic-# : ['Voter', 'vote', 'fraud', 'fraudul']\n",
      "-----\n",
      "#-Topic-# : ['MeToo', 'prejud', 'unbeliv', 'redvirgostaci', 'blackcrim', 'strage', 'rnrkansa', 'protestpoetri', 'dojsuedferguson', 'folllow', 'tswhitesid', 'g-shott', 'notot', 'algiordano', 'stopterror', 'cbeelman', 'nought', 'yetagainimback', 'smittyoneeach', 'nialelkim', 'ableism', 'double-cross', 'repmattgaetz', 'amberaquariu', 'politicaljeff', 'realfrankfromfl', 'trueislam', 'democratg', 'suffragett', 'butlr', 'o2ne1o', 'Weinstein', 'sexual', 'sex', 'harass', 'harassment', 'assault', 'misconduct', 'abus', 'harrass', 'truckitrich', 'sexual', 'alleg', 'predator', 'abus', 'deviant', 'pedophil', 'pervert', 'pedophilia', 'orient', 'feminist', 'femin', 'women', 'sjw']\n",
      "-----\n",
      "#-Topic-# : ['Hurricane', 'irma', 'matthew', 'Irma', 'hurrican', 'matthew', 'matthew', 'hurrican', 'mathew', 'irma', 'flood', 'thunderstorm', 'tornado', 'tsunami', 'quak', 'northeast', 'interst', 'magnitud', 'nepal', 'bermuda', 'blizzard', 'nw', 'reced', 'compart', 'volcano', 'gust', 'earthquak', 'w8', 'fla', 'queensland', 'mph', 'bahama', 'coastlin', 'leve', 'outbreak', 'northwest']\n",
      "-----\n",
      "#-Topic-# : ['Terrorism', 'terrorist', 'jihadist', 'jihad', 'radic', 'terrorterrorist', 'parisattack', 'londonattack', 'shootings', 'shot', 'shooter', 'baltimore', 'chicago', 'milwauke', 'bombings', 'explos', 'bomber', 'Chattanooga', 'puls', 'orlando', 'munich', 'officer-involv', 'istanbul', 'prayforbrussel', 'prayfor', 'prayforbelgium', 'zaventem', 'bruxel', 'vous#prayforbrussel', 'brusselsattack', 'brussel', 'prayfortheworld', 'sprout', 'belgiqu', 'belgium', 'brussel', 'prayfor', 'prayforbelgium', 'prayforbrussel', 'zaventem', 'bruxel', 'belgium', 'massacre', 'nightclub', 'vega']\n",
      "-----\n",
      "#-Topic-# : ['Hacking', 'hacker', 'interfer', 'meddl', 'emails', 'e-mail', 'server', 'document', 'wikileak', 'classifi', 'memo', 'info', 'doc', 'DNC']\n",
      "-----\n",
      "#-Topic-# : ['Obama', 'bho', 'barack', 'barrack', 'hussein', 'michel', 'farewel', 'sasha', 'malia', 'michelle', 'barack', 'malia', 'farewel']\n",
      "-----\n",
      "#-Topic-# : ['Money', 'cash', 'billion', 'fortun', 'scandal', 'WellsFargo', 'nea', 'brasilia', 'migranti', 'pcb', 'nycshutitdown', 'fonseca', 'volkswagen', 'atampt', 'ableg', 'millionsmarch', 'dei', 'lagard', 'espaa', 'annapoli', 'avoir', 'divest', 'mossack', 'chrysler', 'ue', 'subprim', 'out-of-control', 'Fargo', 'ImranAwan', 'Imran', 'awan', 'shultz', 'georgwebb', 'wasserman-schultz', 'wasserman', 'debbi', 'dw', 'Awan', 'imran', 'debbi', 'dw', 'wasserman', 'shultz', 'georgwebb', 'schultz', 'wasserman-schultz', 'Wasserman', 'schultz', 'shultz', 'debbi', 'dw', 'wasserman-schultz', 'awan', 'imran']\n",
      "-----\n",
      "#-Topic-# : ['Election', 'campaign', 'GOP', 'republican', 'dem', 'democrat', 'gopdebate', 'republicandeb', 'undercard', 'abcgop', 'clinton-trump', 'DNC', 'Dem', 'democrat', 'gop', 'republican', 'demdebate', 'vote', 'voter', 'Ivoted', 'debate', 'foxbusinessdeb', 'primary', 'deleg', 'nv', 'caucus', 'nh', 'convention', 'america', 'countri', 'usa', 'amer']\n",
      "-----\n",
      "#-Topic-# : ['Charlottesville', 'alt-righ', 'alt-left', 'fascist', 'anti-fascist', 'alt-right', 'neo-nazi', 'anarchist', 'leftist', 'violent', 'facist', 'antifa', 'neonazi', 'supremist', 'obamasoro', 'ku', 'neo-nazi', 'alt-right', 'anti-semit', 'alt-left', 'antisemit', 'nazi', 'right-w', 'fascist', 'kkk', 'klan', 'klan', 'ku', 'klux', 'kkk', 'supremacist', 'nationalist', 'supremaci', 'supremist', 'supr', 'guilt', 'unitetheright']\n",
      "-----\n",
      "#-Topic-# : ['Economy', 'econom', 'growth', 'consum', 'trillion', 'deal', 'agreement', 'negoti', 'Nafta', 'tpp', 'trade', 'stock', 'dow', 'market', 'soar', 'yuan', 'growth', 'revenu', 'market', 'stock', 'growth', 'optim', 'bitcoin', 'forex', 'blockchain', 'investor', 'cryptocurr', 'currenc', 'verizon', 'revenu', 'hardwar', 'nasdaq', 'dow', 'sampp', 'quarterli', 'aum', 'pace', 'back-to-back', 'belmont', 'yuan', 'canuck', 'record-set', 'ethereum', 'five-year', 'mazda', 'treasuries-u'] \n"
     ]
    }
   ],
   "source": [
    "print('#-Topic-# : {} '.format('\\n-----\\n#-Topic-# : '.join([str(topic) for topic in topic_extended])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
